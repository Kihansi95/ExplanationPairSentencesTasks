{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# All map comparison\n",
    "\n",
    "In this notebook, we make a script that generate HTML comparing the attention map for each data instance, given a folder.\n",
    "\n",
    "## Folder setup\n",
    "\n",
    "We define the folder set as following:\n",
    "\n",
    "```\n",
    "<root>\n",
    "├── ProjectA\n",
    "│   ├── A_map.json\n",
    "│   ├── B_map.json\n",
    "│   ├── C_map.json\n",
    "│   └── ...\n",
    "├── ProjectB\n",
    "└── ...\n",
    "```\n",
    "\n",
    "We want to sample different heatmaps in ProjectA into ProjectA/html. Each output html file will have the file name **<instance_id>.html**\n",
    "\n",
    "We assume that annotation map is found inside of attention_map from models\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21e16826685c6df3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea506cc1c1867fe6"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8135f73-f3f9-4d54-b888-ad08edf3cb7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T12:06:06.721771Z",
     "start_time": "2023-10-13T12:06:06.538057Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "sys.path.append(\"./../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from modules.logger import init_logging\n",
    "from modules.logger import log\n",
    "\n",
    "init_logging(color=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T12:06:12.320500Z",
     "start_time": "2023-10-13T12:06:06.732618Z"
    }
   },
   "id": "7be72d7a281b26a1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f350fd13-a37b-41e2-9514-9a0c0e84e0d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T12:06:12.635972Z",
     "start_time": "2023-10-13T12:06:12.331650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 13 14:06:12 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  On   | 00000000:04:00.0 Off |                  N/A |\r\n",
      "| 23%   19C    P8     7W / 250W |      1MiB / 11178MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9c1ab664bf18052"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-10-2023 14:10:31 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m 4287269976.py:<cell line: 5>:5 \u001B[0m \u001B[34mCurrent node: grele-1.nancy.grid5000.fr\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "# Define root folder based on current node (local or server)\n",
    "node = platform.node()\n",
    "log.info(f'Current node: {node}')\n",
    "if node == 'MAC-C02D80HRMD6':\n",
    "    ROOT = '/Users/dunguyen/Developer/server_backup/historic/2023-06-05'\n",
    "else:\n",
    "    ROOT = '/home/dunguyen/RUNS'\n",
    "    \n",
    "# ==== Choose dataset ====    \n",
    "DATASET = 'yelp-hat'\n",
    "# ========================\n",
    "ROOT = ROOT + '/qualitative_result'\n",
    "PROJECT = f'benchmark_explainers_{DATASET}'\n",
    "MODEL_NAME = 'lstm_attention.run=0_lstm=1'\n",
    "\n",
    "# Define all paths\n",
    "MAPS = [\n",
    "    { 'file_suffix': 'attention_map', 'display': 'Annotation Maps', 'column': 'a_true'},\n",
    "    { 'file_suffix': 'attention_map', 'display': 'Attention Maps', 'column': 'a_hat'},\n",
    "    { 'file_suffix': 'lime_map', 'display': 'LIME Maps', 'column': 'a_lime'},\n",
    "    { 'file_suffix': 'grad_map', 'display': 'Gradient-based Maps', 'column': 'a_grad'},\n",
    "    { 'file_suffix': 'shap_map', 'display': 'SHAP Maps', 'column': 'a_shap'},\n",
    "]\n",
    "\n",
    "# update file path\n",
    "for m in MAPS:\n",
    "    m['fpath'] = path.join(ROOT, PROJECT, MODEL_NAME + '.' + m['file_suffix'] + '.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T12:10:31.261476Z",
     "start_time": "2023-10-13T12:10:30.915370Z"
    }
   },
   "id": "2f8d5563976b6d72"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T12:10:31.857153Z",
     "start_time": "2023-10-13T12:10:31.810823Z"
    }
   },
   "id": "401b2180130fc057"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File /home/dunguyen/RUNS/qualitative_result/benchmark_explainers_yelp-hat/lstm_attention.run=0_lstm=1.attention_map.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Clean padding tokens in attention map files\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_attention \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_json\u001B[49m\u001B[43m(\u001B[49m\u001B[43mMAPS\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfpath\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclean_padding\u001B[39m(row):\n\u001B[1;32m      5\u001B[0m     a_hat \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma_hat\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/virtualenv/eps/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[0;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/virtualenv/eps/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[1;32m    330\u001B[0m     )\n\u001B[0;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/virtualenv/eps/lib/python3.8/site-packages/pandas/io/json/_json.py:733\u001B[0m, in \u001B[0;36mread_json\u001B[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001B[0m\n\u001B[1;32m    730\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_axes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m orient \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    731\u001B[0m     convert_axes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 733\u001B[0m json_reader \u001B[38;5;241m=\u001B[39m \u001B[43mJsonReader\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    734\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    735\u001B[0m \u001B[43m    \u001B[49m\u001B[43morient\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    736\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    737\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    738\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconvert_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    739\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconvert_dates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    740\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkeep_default_dates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_default_dates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    741\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnumpy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    742\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprecise_float\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprecise_float\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    743\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdate_unit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_unit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    744\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    745\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlines\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlines\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    747\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    748\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnrows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnrows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    749\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    750\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    751\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize:\n\u001B[1;32m    754\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m json_reader\n",
      "File \u001B[0;32m~/virtualenv/eps/lib/python3.8/site-packages/pandas/io/json/_json.py:818\u001B[0m, in \u001B[0;36mJsonReader.__init__\u001B[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors)\u001B[0m\n\u001B[1;32m    815\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlines:\n\u001B[1;32m    816\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows can only be passed if lines=True\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 818\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data_from_filepath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    819\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preprocess_data(data)\n",
      "File \u001B[0;32m~/virtualenv/eps/lib/python3.8/site-packages/pandas/io/json/_json.py:874\u001B[0m, in \u001B[0;36mJsonReader._get_data_from_filepath\u001B[0;34m(self, filepath_or_buffer)\u001B[0m\n\u001B[1;32m    866\u001B[0m     filepath_or_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n\u001B[1;32m    867\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m (\n\u001B[1;32m    868\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(filepath_or_buffer, \u001B[38;5;28mstr\u001B[39m)\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m filepath_or_buffer\u001B[38;5;241m.\u001B[39mlower()\u001B[38;5;241m.\u001B[39mendswith(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    872\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m file_exists(filepath_or_buffer)\n\u001B[1;32m    873\u001B[0m ):\n\u001B[0;32m--> 874\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilepath_or_buffer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not exist\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    876\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m filepath_or_buffer\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: File /home/dunguyen/RUNS/qualitative_result/benchmark_explainers_yelp-hat/lstm_attention.run=0_lstm=1.attention_map.json does not exist"
     ]
    }
   ],
   "source": [
    "# Clean padding tokens in attention map files\n",
    "df_attention = pd.read_json(MAPS[1]['fpath'])\n",
    "\n",
    "def clean_padding(row):\n",
    "    a_hat = np.array(row['a_hat'])\n",
    "    padding_mask = np.array(row['padding_mask'])\n",
    "    a_true = np.array(row['a_true'])\n",
    "    a_heu = np.array(row['heuristic'])\n",
    "    tokens = np.array(row['tokens.form'])\n",
    "    a_hat_clean = a_hat[~padding_mask]\n",
    "    a_true_clean = a_true[~padding_mask]\n",
    "    row['a_hat'] = a_hat_clean.tolist()\n",
    "    row['a_true'] = a_true_clean.tolist()\n",
    "    row['heuristic'] = a_heu[~padding_mask].tolist()\n",
    "    # row['tokens.form'] = tokens[~padding_mask].tolist()\n",
    "    return row\n",
    "\n",
    "if 'padding_mask' in df_attention.columns:\n",
    "    df_attention = df_attention.apply(clean_padding, axis=1)\n",
    "    df_attention = df_attention.drop(columns=['padding_mask'])\n",
    "    df_attention.to_json(MAPS[1]['fpath'])\n",
    "    \n",
    "# Replace label \n",
    "if 'label_hat' not in df_attention.columns:\n",
    "    label_itos = dict()\n",
    "    if DATASET == 'hatexplain': \n",
    "        from data.hatexplain.dataset import HateXPlain\n",
    "        label_itos = HateXPlain.LABEL_ITOS\n",
    "    elif DATASET == 'yelphat':\n",
    "        from data.yelp_hat.dataset import YelpHat\n",
    "        label_itos = YelpHat.LABEL_ITOS\n",
    "    elif DATASET == 'esnli':\n",
    "        from data.esnli.dataset import ESNLI\n",
    "        label_itos = ESNLI.LABEL_ITOS\n",
    "    else:\n",
    "        raise ValueError('Dataset not supported')\n",
    "    \n",
    "    #df_attention['label_hat'] = df_attention['y_hat'].apply(lambda x: label_itos[x])\n",
    "    #df_attention['label_true'] = df_attention['y_true'].apply(lambda x: label_itos[x])\n",
    "    df_attention['label_hat'] = df_attention['y_hat']\n",
    "    df_attention['label_true'] = df_attention['y_hat']\n",
    "    df_attention.to_json(MAPS[1]['fpath'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T12:10:32.630131Z",
     "start_time": "2023-10-13T12:10:32.097864Z"
    }
   },
   "id": "50994c0415733843"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_attention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 38>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     35\u001B[0m         df_attention \u001B[38;5;241m=\u001B[39m df_attention\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokens.norm.premise\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokens.norm.hypothesis\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     36\u001B[0m         df_attention\u001B[38;5;241m.\u001B[39mto_json(MAPS[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfpath\u001B[39m\u001B[38;5;124m'\u001B[39m])    \n\u001B[0;32m---> 38\u001B[0m \u001B[43mdf_attention\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_attention' is not defined"
     ]
    }
   ],
   "source": [
    "# Treating eSNLI: fusion all together\n",
    "def clean_padding_nli(row):\n",
    "    \"\"\"Clean padding tokens in attention map files\"\"\"\n",
    "    for side in ['premise', 'hypothesis']:\n",
    "        padding_mask = np.array(row['padding_mask.'+side])\n",
    "        a_true = np.array(row['a_true.'+side])\n",
    "        a_hat = np.array(row['a_hat.'+side])\n",
    "        row['a_true.'+side] = a_true[~padding_mask].tolist()\n",
    "        row['a_hat.'+side] = a_hat[~padding_mask].tolist()\n",
    "    return row\n",
    "\n",
    "if DATASET == 'esnli':\n",
    "    \n",
    "    # Clean padding mask in premise and hypothesis\n",
    "    if 'padding_mask.premise' in df_attention.columns:\n",
    "        log.debug(f'Cleaning padding tokens for eSNLI')\n",
    "        df_attention = df_attention.apply(clean_padding_nli, axis=1)\n",
    "        df_attention = df_attention.drop(columns=['padding_mask.premise', 'padding_mask.hypothesis'])\n",
    "        df_attention.to_json(MAPS[1]['fpath'])\n",
    "    \n",
    "    # Normalize weights if this is not done in attention map\n",
    "    max_vector = df_attention['a_hat.premise'].apply(lambda x: max(x))\n",
    "    if (max_vector < 1).any():\n",
    "        from modules.utils import rescale\n",
    "        log.debug(f'Normalize attention map for eSNLI')\n",
    "        df_attention['a_hat.premise'] = df_attention['a_hat.premise'].apply(lambda x: rescale(x).tolist())\n",
    "        df_attention['a_hat.hypothesis'] = df_attention['a_hat.hypothesis'].apply(lambda x: rescale(x).tolist())\n",
    "        df_attention.to_json(MAPS[1]['fpath'])\n",
    "        \n",
    "    # concatenate tokens\n",
    "    if 'tokens.form' not in df_attention.columns:  \n",
    "        log.debug(f'Concat tokens for eSNLI')\n",
    "        # TODO: change back to tokens.form once this is fixed\n",
    "        df_attention['tokens.form'] = df_attention.apply(lambda row: ['<b>Premise</b>:'] + row['tokens.norm.premise'] + ['<br/><b>Hypothesis</b>:'] + row['tokens.norm.hypothesis'], axis=1)\n",
    "        df_attention = df_attention.drop(columns=['tokens.norm.premise', 'tokens.norm.hypothesis'])\n",
    "        df_attention.to_json(MAPS[1]['fpath'])    \n",
    "    \n",
    "df_attention"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T12:06:21.891280Z",
     "start_time": "2023-10-13T12:06:21.010968Z"
    }
   },
   "id": "36b2796ae1fe7f64"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                   tokens.form  \\\nid                                                                               \n14570598_gab                 [i, have, read, about, this, it, typical, nigg...   \n22341782_gab                                             [was, <unk>, a, kike]   \n1122915768600072193_twitter  [<user>, they, just, some, hating, hoes, tho, ...   \n16544694_gab                 [indeed, <unk>, football, is, as, corrupt, of,...   \n12976797_gab                        [we, need, common, sense, nigger, control]   \n...                                                                        ...   \n2456779_gab                  [<user>, you, can, take, the, family, out, of,...   \n1123227111810945024_twitter                               [lmao, gay, haircut]   \n16921321_gab                                       [lol, good, nigger, <user>]   \n17684585_gab                 [she, is, tired, of, liberal, faggots, <unk>, ...   \n11443584_gab                 [throwing, <unk>, is, a, bitch, move, it, what...   \n\n                              label_hat  label_true  y_hat  y_true  \\\nid                                                                   \n14570598_gab                 hatespeech  hatespeech      1       1   \n22341782_gab                 hatespeech  hatespeech      1       1   \n1122915768600072193_twitter   offensive   offensive      2       2   \n16544694_gab                  offensive   offensive      2       2   \n12976797_gab                 hatespeech  hatespeech      1       1   \n...                                 ...         ...    ...     ...   \n2456779_gab                   offensive   offensive      2       2   \n1123227111810945024_twitter   offensive   offensive      2       2   \n16921321_gab                 hatespeech  hatespeech      1       1   \n17684585_gab                  offensive   offensive      2       2   \n11443584_gab                  offensive   offensive      2       2   \n\n                                                                        a_true  \\\nid                                                                               \n14570598_gab                 [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n22341782_gab                                                      [0, 0, 0, 1]   \n1122915768600072193_twitter                           [0, 0, 0, 0, 0, 1, 0, 0]   \n16544694_gab                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n12976797_gab                                                [0, 0, 0, 0, 1, 0]   \n...                                                                        ...   \n2456779_gab                  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n1123227111810945024_twitter                                          [0, 1, 0]   \n16921321_gab                                                      [0, 0, 1, 0]   \n17684585_gab                 [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n11443584_gab                 [0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, ...   \n\n                                                                         a_hat  \\\nid                                                                               \n14570598_gab                 [0.00024549695081077516, 0.0012934240512549877...   \n22341782_gab                 [0.0, 0.017309796065092087, 0.0190470293164253...   \n1122915768600072193_twitter  [0.16280816495418549, 0.00742510287091136, 0.0...   \n16544694_gab                 [0.013485023751854897, 0.07245766371488571, 0....   \n12976797_gab                 [0.0, 0.005834614392369986, 0.0070640081539750...   \n...                                                                        ...   \n2456779_gab                  [0.2856060862541199, 0.04960240051150322, 0.02...   \n1123227111810945024_twitter                       [1.0, 0.0, 0.99031662940979]   \n16921321_gab                 [0.0, 0.008781231939792633, 1.0, 0.23454540967...   \n17684585_gab                 [0.035775136202573776, 0.028469529002904892, 0...   \n11443584_gab                 [0.5293900370597839, 0.5734730958938599, 0.231...   \n\n                                                                        a_lime  \\\nid                                                                               \n14570598_gab                 [0.1144649732, 0.032017954200000004, 0.0079438...   \n22341782_gab                                              [0.0, 0.0, 0.0, 1.0]   \n1122915768600072193_twitter  [0.0, 0.108021086, 0.1053259277, 0.0650216893,...   \n16544694_gab                 [0.36321024360000004, 0.0, 0.0410238744, 0.126...   \n12976797_gab                 [0.0291292242, 0.0210062664, 0.0, 0.0, 1.0, 0....   \n...                                                                        ...   \n2456779_gab                  [0.0, 0.1632703204, 0.0485266852, 0.0403184933...   \n1123227111810945024_twitter                           [0.1784834981, 0.0, 1.0]   \n16921321_gab                                     [0.0164801053, 0.0, 1.0, 0.0]   \n17684585_gab                 [0.54890555, 0.2665352696, 0.1566093943, 0.024...   \n11443584_gab                 [0.022061997, 0.0, 0.1226110871, 0.0959635749,...   \n\n                                                                        a_grad  \\\nid                                                                               \n14570598_gab                 [0.3386931121, 0.191345498, 0.1753299385, 0.15...   \n22341782_gab                            [0.0210656822, 0.0, 0.0831173882, 1.0]   \n1122915768600072193_twitter  [0.1504608393, 0.0916632861, 0.1263468713, 0.2...   \n16544694_gab                 [0.5256419182000001, 0.3982704282, 0.264808476...   \n12976797_gab                 [0.0720760673, 0.0, 0.024250973000000002, 0.12...   \n...                                                                        ...   \n2456779_gab                  [0.1890623122, 0.0710416287, 0.0231457576, 0.0...   \n1123227111810945024_twitter                           [1.0, 0.4529289305, 0.0]   \n16921321_gab                     [0.08586186920000001, 0.0830580294, 1.0, 0.0]   \n17684585_gab                 [0.3942760229, 0.2884926796, 0.3066299856, 0.3...   \n11443584_gab                 [0.6553323269, 0.5915272236, 0.4967728853, 0.4...   \n\n                                                                        a_shap  \nid                                                                              \n14570598_gab                 [0.14126749530000002, 0.07320565350000001, 0.0...  \n22341782_gab                 [0.0, 0.053090922900000004, 0.0232919261000000...  \n1122915768600072193_twitter  [0.12905111730000002, 0.1060431673, 0.09427566...  \n16544694_gab                 [0.1801965483, 0.039558463700000004, 0.0227379...  \n12976797_gab                 [0.0636289999, 0.0743818118, 0.030693171600000...  \n...                                                                        ...  \n2456779_gab                  [0.0595646216, 0.2216111359, 0.1637617995, 0.1...  \n1123227111810945024_twitter                    [0.40109014460000003, 0.0, 1.0]  \n16921321_gab                       [0.05753767, 0.08117362830000001, 1.0, 0.0]  \n17684585_gab                 [0.3516586936, 0.2595858796, 0.1582157686, 0.1...  \n11443584_gab                 [0.08617778940000001, 0.0732300109, 0.14913725...  \n\n[749 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tokens.form</th>\n      <th>label_hat</th>\n      <th>label_true</th>\n      <th>y_hat</th>\n      <th>y_true</th>\n      <th>a_true</th>\n      <th>a_hat</th>\n      <th>a_lime</th>\n      <th>a_grad</th>\n      <th>a_shap</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14570598_gab</th>\n      <td>[i, have, read, about, this, it, typical, nigg...</td>\n      <td>hatespeech</td>\n      <td>hatespeech</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0.00024549695081077516, 0.0012934240512549877...</td>\n      <td>[0.1144649732, 0.032017954200000004, 0.0079438...</td>\n      <td>[0.3386931121, 0.191345498, 0.1753299385, 0.15...</td>\n      <td>[0.14126749530000002, 0.07320565350000001, 0.0...</td>\n    </tr>\n    <tr>\n      <th>22341782_gab</th>\n      <td>[was, &lt;unk&gt;, a, kike]</td>\n      <td>hatespeech</td>\n      <td>hatespeech</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[0, 0, 0, 1]</td>\n      <td>[0.0, 0.017309796065092087, 0.0190470293164253...</td>\n      <td>[0.0, 0.0, 0.0, 1.0]</td>\n      <td>[0.0210656822, 0.0, 0.0831173882, 1.0]</td>\n      <td>[0.0, 0.053090922900000004, 0.0232919261000000...</td>\n    </tr>\n    <tr>\n      <th>1122915768600072193_twitter</th>\n      <td>[&lt;user&gt;, they, just, some, hating, hoes, tho, ...</td>\n      <td>offensive</td>\n      <td>offensive</td>\n      <td>2</td>\n      <td>2</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n      <td>[0.16280816495418549, 0.00742510287091136, 0.0...</td>\n      <td>[0.0, 0.108021086, 0.1053259277, 0.0650216893,...</td>\n      <td>[0.1504608393, 0.0916632861, 0.1263468713, 0.2...</td>\n      <td>[0.12905111730000002, 0.1060431673, 0.09427566...</td>\n    </tr>\n    <tr>\n      <th>16544694_gab</th>\n      <td>[indeed, &lt;unk&gt;, football, is, as, corrupt, of,...</td>\n      <td>offensive</td>\n      <td>offensive</td>\n      <td>2</td>\n      <td>2</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0.013485023751854897, 0.07245766371488571, 0....</td>\n      <td>[0.36321024360000004, 0.0, 0.0410238744, 0.126...</td>\n      <td>[0.5256419182000001, 0.3982704282, 0.264808476...</td>\n      <td>[0.1801965483, 0.039558463700000004, 0.0227379...</td>\n    </tr>\n    <tr>\n      <th>12976797_gab</th>\n      <td>[we, need, common, sense, nigger, control]</td>\n      <td>hatespeech</td>\n      <td>hatespeech</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[0, 0, 0, 0, 1, 0]</td>\n      <td>[0.0, 0.005834614392369986, 0.0070640081539750...</td>\n      <td>[0.0291292242, 0.0210062664, 0.0, 0.0, 1.0, 0....</td>\n      <td>[0.0720760673, 0.0, 0.024250973000000002, 0.12...</td>\n      <td>[0.0636289999, 0.0743818118, 0.030693171600000...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2456779_gab</th>\n      <td>[&lt;user&gt;, you, can, take, the, family, out, of,...</td>\n      <td>offensive</td>\n      <td>offensive</td>\n      <td>2</td>\n      <td>2</td>\n      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[0.2856060862541199, 0.04960240051150322, 0.02...</td>\n      <td>[0.0, 0.1632703204, 0.0485266852, 0.0403184933...</td>\n      <td>[0.1890623122, 0.0710416287, 0.0231457576, 0.0...</td>\n      <td>[0.0595646216, 0.2216111359, 0.1637617995, 0.1...</td>\n    </tr>\n    <tr>\n      <th>1123227111810945024_twitter</th>\n      <td>[lmao, gay, haircut]</td>\n      <td>offensive</td>\n      <td>offensive</td>\n      <td>2</td>\n      <td>2</td>\n      <td>[0, 1, 0]</td>\n      <td>[1.0, 0.0, 0.99031662940979]</td>\n      <td>[0.1784834981, 0.0, 1.0]</td>\n      <td>[1.0, 0.4529289305, 0.0]</td>\n      <td>[0.40109014460000003, 0.0, 1.0]</td>\n    </tr>\n    <tr>\n      <th>16921321_gab</th>\n      <td>[lol, good, nigger, &lt;user&gt;]</td>\n      <td>hatespeech</td>\n      <td>hatespeech</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[0, 0, 1, 0]</td>\n      <td>[0.0, 0.008781231939792633, 1.0, 0.23454540967...</td>\n      <td>[0.0164801053, 0.0, 1.0, 0.0]</td>\n      <td>[0.08586186920000001, 0.0830580294, 1.0, 0.0]</td>\n      <td>[0.05753767, 0.08117362830000001, 1.0, 0.0]</td>\n    </tr>\n    <tr>\n      <th>17684585_gab</th>\n      <td>[she, is, tired, of, liberal, faggots, &lt;unk&gt;, ...</td>\n      <td>offensive</td>\n      <td>offensive</td>\n      <td>2</td>\n      <td>2</td>\n      <td>[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0.035775136202573776, 0.028469529002904892, 0...</td>\n      <td>[0.54890555, 0.2665352696, 0.1566093943, 0.024...</td>\n      <td>[0.3942760229, 0.2884926796, 0.3066299856, 0.3...</td>\n      <td>[0.3516586936, 0.2595858796, 0.1582157686, 0.1...</td>\n    </tr>\n    <tr>\n      <th>11443584_gab</th>\n      <td>[throwing, &lt;unk&gt;, is, a, bitch, move, it, what...</td>\n      <td>offensive</td>\n      <td>offensive</td>\n      <td>2</td>\n      <td>2</td>\n      <td>[0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n      <td>[0.5293900370597839, 0.5734730958938599, 0.231...</td>\n      <td>[0.022061997, 0.0, 0.1226110871, 0.0959635749,...</td>\n      <td>[0.6553323269, 0.5915272236, 0.4967728853, 0.4...</td>\n      <td>[0.08617778940000001, 0.0732300109, 0.14913725...</td>\n    </tr>\n  </tbody>\n</table>\n<p>749 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and fusion into a single dataframe\n",
    "map_data = None\n",
    "for m in MAPS:\n",
    "    # load data from json file\n",
    "    df = pd.read_json(m['fpath'])\n",
    "    df.set_index('id', inplace=True)\n",
    "    \n",
    "    column = m['column']\n",
    "    \n",
    "    # concat if this is esnli\n",
    "    if DATASET == 'esnli':\n",
    "        from modules.utils import rescale\n",
    "        if (column != 'a_true') and (df[column+'.premise'].apply(lambda x: max(x)) != 1).any():\n",
    "            df[column+'.premise'] = df[column+'.premise'].apply(lambda x: rescale(x).tolist())\n",
    "            df[column+'.hypothesis'] = df[column+'.hypothesis'].apply(lambda x: rescale(x).tolist())\n",
    "        df[column] = df.apply(lambda row: [0] + row[column +'.premise'] + [0] + row[column+'.hypothesis'], axis=1)\n",
    "        df.drop(columns=[column +'.premise', column+'.hypothesis'], inplace=True)\n",
    "        \n",
    "    else:\n",
    "        #if not e-SNLI, normalize the weight\n",
    "        if (column != 'a_true') and (df[column].apply(lambda x: max(x)) != 1).any():\n",
    "            from modules.utils import rescale\n",
    "            df[column] = df[column].apply(lambda x: rescale(x).tolist())\n",
    "\n",
    "    # the first dataframe will query the id and the tokens\n",
    "    if map_data is None:\n",
    "        map_data = df[['tokens.form', 'label_hat', 'label_true', 'y_hat', 'y_true']].copy()\n",
    "\n",
    "    map_data = map_data.join(df[column])\n",
    "\n",
    "map_data = map_data[(map_data['y_hat'] == map_data['y_true']) & (map_data['y_hat'] != 0)]\n",
    "map_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T12:06:43.831766Z",
     "start_time": "2023-10-13T12:06:43.041350Z"
    }
   },
   "id": "dd1b125b20f59dae"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def steep_sigmoid(x, s=10, p=2):\n",
    "    x = np.array(x)\n",
    "    x_normalized = 1 / (1 + np.exp(-s * (x - 0.5)))**p\n",
    "    return x_normalized.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T12:06:44.696370Z",
     "start_time": "2023-10-13T12:06:44.607777Z"
    }
   },
   "id": "bc202aacf9da1f4f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/749 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b86d62f841b4b8ab771a9a16e011312"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from modules.utils import highlight\n",
    "import shutil\n",
    "\n",
    "# Remove the previous existing folder\n",
    "html_dir = path.join(ROOT, PROJECT, '.html')\n",
    "if os.path.exists(html_dir) and os.path.isdir(html_dir):\n",
    "    log.info(f'Removing existing folder {html_dir}')\n",
    "    shutil.rmtree(html_dir)\n",
    "\n",
    "# Generate each comparison into a file:\n",
    "for idx, row in tqdm(map_data.iterrows(), total=len(map_data)):\n",
    "    \n",
    "    # ignore if label is 0\n",
    "    if row['y_true'] == 0: continue\n",
    "    \n",
    "    # ignore if row contains any NaN\n",
    "    if row.isnull().sum() > 0: continue\n",
    "    \n",
    "    html = \"\"\"\n",
    "    <html>\n",
    "    <head><style>\n",
    "    table, th, td {\n",
    "      border:solid black;\n",
    "      border-collapse: collapse;\n",
    "      padding: 0px 5px 0px 5px;\n",
    "    }</style></head>\n",
    "    <body>\n",
    "    \"\"\"\n",
    "    html += '<table style=\"font-size:120%;\" cellspacing=0>'\n",
    "    html += f'<caption>Dataset: {DATASET} - Instance ID: {idx}</caption>'\n",
    "    html += '<tr><th style=\"width:100px;\">Explainer</th> <th style=\"width:500px;\">Explanation</th> <th style=\"width:100px;\">Predicted label</th> <th style=\"width:100px;\">True label</th></tr>'\n",
    "    \n",
    "    # Display a row for each map\n",
    "    for m in MAPS:\n",
    "        html += '<tr>'\n",
    "        \n",
    "        # Display the explainer and its explanation\n",
    "        c = m['column']\n",
    "        map_name = m['display']\n",
    "        # TODO check what if we change the value in gradient map:\n",
    "        #if c == 'a_grad':\n",
    "        #    row[c] = steep_sigmoid(row[c], s=5, p=2)\n",
    "        map_viz = highlight(row['tokens.form'], row[c], normalize_weight=False)\n",
    "        html+= f'<td style=\"text-align:right;\"> {map_name} </td><td> {map_viz} </td>'\n",
    "        \n",
    "        # For the first row, display the spanning the label\n",
    "        if c == 'a_true':\n",
    "            row_span = len(MAPS)\n",
    "            html +=f'<td rowspan=\"{row_span}\" style=\"text-align:center\"> {row[\"label_hat\"]} </td>'\n",
    "            html +=f'<td rowspan=\"{row_span}\" style=\"text-align:center\"> {row[\"label_true\"]} </td>'\n",
    "            \n",
    "        html += '</tr>\\n'\n",
    "        \n",
    "    html += '</table>'\n",
    "    html += '</body></html>'\n",
    "\n",
    "    fpath_html = path.join(html_dir, f'{idx}.html')\n",
    "    os.makedirs(html_dir, exist_ok=True)\n",
    "    with open(fpath_html, 'w') as f:\n",
    "        f.write(html)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T12:06:46.368547Z",
     "start_time": "2023-10-13T12:06:44.882855Z"
    }
   },
   "id": "c7cfd6cf85424f96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modify dataset columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44de778dfcb7f658"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                    id                                            premise  \\\n0  2677109430.jpg#1r1n  This church choir sings to the masses as they ...   \n1  2677109430.jpg#1r1e  This church choir sings to the masses as they ...   \n2  2677109430.jpg#1r1c  This church choir sings to the masses as they ...   \n3  6160193920.jpg#4r1n  A woman with a green headscarf, blue shirt and...   \n4  6160193920.jpg#4r1e  A woman with a green headscarf, blue shirt and...   \n5  6160193920.jpg#4r1c  A woman with a green headscarf, blue shirt and...   \n6  4791890474.jpg#3r1e  An old man with a package poses in front of an...   \n7  4791890474.jpg#3r1n  An old man with a package poses in front of an...   \n8  4791890474.jpg#3r1c  An old man with a package poses in front of an...   \n9  6526219567.jpg#4r1n  A statue at a museum that no seems to be looki...   \n\n                                          hypothesis          label  \\\n0              The church has cracks in the ceiling.        neutral   \n1                    The church is filled with song.     entailment   \n2                A choir singing at a baseball game.  contradiction   \n3                                The woman is young.        neutral   \n4                           The woman is very happy.     entailment   \n5                           The woman has been shot.  contradiction   \n6                     A man poses in front of an ad.     entailment   \n7            A man poses in front of an ad for beer.        neutral   \n8                              A man walks by an ad.  contradiction   \n9  The statue is offensive and people are mad tha...        neutral   \n\n                                         explanation  \\\n0        Not all churches have cracks in the ceiling   \n1  \"Filled with song\" is a rephrasing of the \"cho...   \n2  A choir sing some other songs other than book ...   \n3      the woman could've been old rather than young   \n4                          a grin suggests hapiness.   \n5  There can be either a woman with a very big gr...   \n6  The word \" ad \" is short for the word \" advert...   \n7            Not all advertisements are ad for beer.   \n8  The man poses in front of the advertisement th...   \n9  Not all statues are ignored because they are o...   \n\n                                   highlight_premise  \\\n0  This church choir sings to the masses as they ...   \n1  This church *choir* *sings* *to* *the* *masses...   \n2  This church choir sings to the *masses* as the...   \n3  A woman with a green headscarf, blue shirt and...   \n4  A woman with a green headscarf, blue shirt and...   \n5  A woman with a *green* headscarf, blue shirt a...   \n6  An old man with a package poses in front of an...   \n7  An old man with a package poses in front of an...   \n8  An old *man* with a package *poses* *in* *fron...   \n9  A statue at a museum that no seems to be looki...   \n\n                                highlight_hypothesis  \\\n0      The church has *cracks* *in* *the* *ceiling.*   \n1              The church is *filled* *with* *song.*   \n2          A choir *singing* at a *baseball* *game.*   \n3                              The woman is *young.*   \n4                         The woman is very *happy.*   \n5                         The woman has been *shot.*   \n6                   A man poses in front of an *ad.*   \n7          A man poses in front of an ad for *beer.*   \n8                        A man *walks* *by* an *ad.*   \n9  The statue is *offensive* and people are mad t...   \n\n                                 tokens.norm.premise  \\\n0  [this, church, choir, sing, to, the, masse, as...   \n1  [this, church, choir, sing, to, the, masse, as...   \n2  [this, church, choir, sing, to, the, masse, as...   \n3  [a, woman, with, a, green, headscarf, ,, blue,...   \n4  [a, woman, with, a, green, headscarf, ,, blue,...   \n5  [a, woman, with, a, green, headscarf, ,, blue,...   \n6  [an, old, man, with, a, package, pose, in, fro...   \n7  [an, old, man, with, a, package, pose, in, fro...   \n8  [an, old, man, with, a, package, pose, in, fro...   \n9  [a, statue, at, a, museum, that, no, seem, to,...   \n\n                              tokens.norm.hypothesis  \\\n0    [the, church, have, crack, in, the, ceiling, .]   \n1             [the, church, be, fill, with, song, .]   \n2      [a, choir, singing, at, a, baseball, game, .]   \n3                         [the, woman, be, young, .]   \n4                   [the, woman, be, very, happy, .]   \n5                   [the, woman, have, be, shoot, .]   \n6           [a, man, pose, in, front, of, an, ad, .]   \n7  [a, man, pose, in, front, of, an, ad, for, bee...   \n8                      [a, man, walk, by, an, ad, .]   \n9  [the, statue, be, offensive, and, people, be, ...   \n\n                                   rationale.premise  \\\n0  [False, False, False, False, False, False, Fal...   \n1  [False, False, True, True, True, True, True, F...   \n2  [False, False, False, False, False, False, Tru...   \n3  [False, False, False, False, False, False, Fal...   \n4  [False, False, False, False, False, False, Fal...   \n5  [False, False, False, False, True, False, Fals...   \n6  [False, False, False, False, False, False, Fal...   \n7  [False, False, False, False, False, False, Fal...   \n8  [False, False, True, False, False, False, True...   \n9  [False, False, False, False, False, False, Fal...   \n\n                                rationale.hypothesis  \\\n0  [False, False, False, True, True, True, True, ...   \n1     [False, False, False, True, True, True, False]   \n2  [False, False, True, False, False, True, True,...   \n3                 [False, False, False, True, False]   \n4          [False, False, False, False, True, False]   \n5          [False, False, False, False, True, False]   \n6  [False, False, False, False, False, False, Fal...   \n7  [False, False, False, False, False, False, Fal...   \n8     [False, False, True, True, False, True, False]   \n9  [False, False, False, True, False, False, Fals...   \n\n                                   heuristic.premise  \\\n0  [-1.0000000150474662e+30, 3.064525842666626, 1...   \n1  [-1.0000000150474662e+30, 2.79181170463562, 2....   \n2  [-1.0000000150474662e+30, 2.5598974227905273, ...   \n3  [-1.0000000150474662e+30, 2.597653388977051, -...   \n4  [-1.0000000150474662e+30, 2.784580707550049, -...   \n5  [-1.0000000150474662e+30, 2.6564526557922363, ...   \n6  [-1.0000000150474662e+30, 2.9205048084259033, ...   \n7  [-1.0000000150474662e+30, 3.5127861499786377, ...   \n8  [-1.0000000150474662e+30, 2.2357261180877686, ...   \n9  [-1.0000000150474662e+30, 3.6017332077026367, ...   \n\n                                heuristic.hypothesis  \n0  [-1.0000000150474662e+30, 7.628961086273193, -...  \n1  [-1.0000000150474662e+30, 7.628961086273193, -...  \n2  [-1.0000000150474662e+30, 6.388305187225342, 6...  \n3  [-1.0000000150474662e+30, 5.648240089416504, -...  \n4  [-1.0000000150474662e+30, 5.648240089416504, -...  \n5  [-1.0000000150474662e+30, 5.648240089416504, -...  \n6  [-1.0000000150474662e+30, 5.1345367431640625, ...  \n7  [-1.0000000150474662e+30, 5.1345367431640625, ...  \n8  [-1.0000000150474662e+30, 5.1345367431640625, ...  \n9  [-1.0000000150474662e+30, 3.75215744972229, -1...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n      <th>explanation</th>\n      <th>highlight_premise</th>\n      <th>highlight_hypothesis</th>\n      <th>tokens.norm.premise</th>\n      <th>tokens.norm.hypothesis</th>\n      <th>rationale.premise</th>\n      <th>rationale.hypothesis</th>\n      <th>heuristic.premise</th>\n      <th>heuristic.hypothesis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2677109430.jpg#1r1n</td>\n      <td>This church choir sings to the masses as they ...</td>\n      <td>The church has cracks in the ceiling.</td>\n      <td>neutral</td>\n      <td>Not all churches have cracks in the ceiling</td>\n      <td>This church choir sings to the masses as they ...</td>\n      <td>The church has *cracks* *in* *the* *ceiling.*</td>\n      <td>[this, church, choir, sing, to, the, masse, as...</td>\n      <td>[the, church, have, crack, in, the, ceiling, .]</td>\n      <td>[False, False, False, False, False, False, Fal...</td>\n      <td>[False, False, False, True, True, True, True, ...</td>\n      <td>[-1.0000000150474662e+30, 3.064525842666626, 1...</td>\n      <td>[-1.0000000150474662e+30, 7.628961086273193, -...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2677109430.jpg#1r1e</td>\n      <td>This church choir sings to the masses as they ...</td>\n      <td>The church is filled with song.</td>\n      <td>entailment</td>\n      <td>\"Filled with song\" is a rephrasing of the \"cho...</td>\n      <td>This church *choir* *sings* *to* *the* *masses...</td>\n      <td>The church is *filled* *with* *song.*</td>\n      <td>[this, church, choir, sing, to, the, masse, as...</td>\n      <td>[the, church, be, fill, with, song, .]</td>\n      <td>[False, False, True, True, True, True, True, F...</td>\n      <td>[False, False, False, True, True, True, False]</td>\n      <td>[-1.0000000150474662e+30, 2.79181170463562, 2....</td>\n      <td>[-1.0000000150474662e+30, 7.628961086273193, -...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2677109430.jpg#1r1c</td>\n      <td>This church choir sings to the masses as they ...</td>\n      <td>A choir singing at a baseball game.</td>\n      <td>contradiction</td>\n      <td>A choir sing some other songs other than book ...</td>\n      <td>This church choir sings to the *masses* as the...</td>\n      <td>A choir *singing* at a *baseball* *game.*</td>\n      <td>[this, church, choir, sing, to, the, masse, as...</td>\n      <td>[a, choir, singing, at, a, baseball, game, .]</td>\n      <td>[False, False, False, False, False, False, Tru...</td>\n      <td>[False, False, True, False, False, True, True,...</td>\n      <td>[-1.0000000150474662e+30, 2.5598974227905273, ...</td>\n      <td>[-1.0000000150474662e+30, 6.388305187225342, 6...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6160193920.jpg#4r1n</td>\n      <td>A woman with a green headscarf, blue shirt and...</td>\n      <td>The woman is young.</td>\n      <td>neutral</td>\n      <td>the woman could've been old rather than young</td>\n      <td>A woman with a green headscarf, blue shirt and...</td>\n      <td>The woman is *young.*</td>\n      <td>[a, woman, with, a, green, headscarf, ,, blue,...</td>\n      <td>[the, woman, be, young, .]</td>\n      <td>[False, False, False, False, False, False, Fal...</td>\n      <td>[False, False, False, True, False]</td>\n      <td>[-1.0000000150474662e+30, 2.597653388977051, -...</td>\n      <td>[-1.0000000150474662e+30, 5.648240089416504, -...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6160193920.jpg#4r1e</td>\n      <td>A woman with a green headscarf, blue shirt and...</td>\n      <td>The woman is very happy.</td>\n      <td>entailment</td>\n      <td>a grin suggests hapiness.</td>\n      <td>A woman with a green headscarf, blue shirt and...</td>\n      <td>The woman is very *happy.*</td>\n      <td>[a, woman, with, a, green, headscarf, ,, blue,...</td>\n      <td>[the, woman, be, very, happy, .]</td>\n      <td>[False, False, False, False, False, False, Fal...</td>\n      <td>[False, False, False, False, True, False]</td>\n      <td>[-1.0000000150474662e+30, 2.784580707550049, -...</td>\n      <td>[-1.0000000150474662e+30, 5.648240089416504, -...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6160193920.jpg#4r1c</td>\n      <td>A woman with a green headscarf, blue shirt and...</td>\n      <td>The woman has been shot.</td>\n      <td>contradiction</td>\n      <td>There can be either a woman with a very big gr...</td>\n      <td>A woman with a *green* headscarf, blue shirt a...</td>\n      <td>The woman has been *shot.*</td>\n      <td>[a, woman, with, a, green, headscarf, ,, blue,...</td>\n      <td>[the, woman, have, be, shoot, .]</td>\n      <td>[False, False, False, False, True, False, Fals...</td>\n      <td>[False, False, False, False, True, False]</td>\n      <td>[-1.0000000150474662e+30, 2.6564526557922363, ...</td>\n      <td>[-1.0000000150474662e+30, 5.648240089416504, -...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4791890474.jpg#3r1e</td>\n      <td>An old man with a package poses in front of an...</td>\n      <td>A man poses in front of an ad.</td>\n      <td>entailment</td>\n      <td>The word \" ad \" is short for the word \" advert...</td>\n      <td>An old man with a package poses in front of an...</td>\n      <td>A man poses in front of an *ad.*</td>\n      <td>[an, old, man, with, a, package, pose, in, fro...</td>\n      <td>[a, man, pose, in, front, of, an, ad, .]</td>\n      <td>[False, False, False, False, False, False, Fal...</td>\n      <td>[False, False, False, False, False, False, Fal...</td>\n      <td>[-1.0000000150474662e+30, 2.9205048084259033, ...</td>\n      <td>[-1.0000000150474662e+30, 5.1345367431640625, ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>4791890474.jpg#3r1n</td>\n      <td>An old man with a package poses in front of an...</td>\n      <td>A man poses in front of an ad for beer.</td>\n      <td>neutral</td>\n      <td>Not all advertisements are ad for beer.</td>\n      <td>An old man with a package poses in front of an...</td>\n      <td>A man poses in front of an ad for *beer.*</td>\n      <td>[an, old, man, with, a, package, pose, in, fro...</td>\n      <td>[a, man, pose, in, front, of, an, ad, for, bee...</td>\n      <td>[False, False, False, False, False, False, Fal...</td>\n      <td>[False, False, False, False, False, False, Fal...</td>\n      <td>[-1.0000000150474662e+30, 3.5127861499786377, ...</td>\n      <td>[-1.0000000150474662e+30, 5.1345367431640625, ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>4791890474.jpg#3r1c</td>\n      <td>An old man with a package poses in front of an...</td>\n      <td>A man walks by an ad.</td>\n      <td>contradiction</td>\n      <td>The man poses in front of the advertisement th...</td>\n      <td>An old *man* with a package *poses* *in* *fron...</td>\n      <td>A man *walks* *by* an *ad.*</td>\n      <td>[an, old, man, with, a, package, pose, in, fro...</td>\n      <td>[a, man, walk, by, an, ad, .]</td>\n      <td>[False, False, True, False, False, False, True...</td>\n      <td>[False, False, True, True, False, True, False]</td>\n      <td>[-1.0000000150474662e+30, 2.2357261180877686, ...</td>\n      <td>[-1.0000000150474662e+30, 5.1345367431640625, ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>6526219567.jpg#4r1n</td>\n      <td>A statue at a museum that no seems to be looki...</td>\n      <td>The statue is offensive and people are mad tha...</td>\n      <td>neutral</td>\n      <td>Not all statues are ignored because they are o...</td>\n      <td>A statue at a museum that no seems to be looki...</td>\n      <td>The statue is *offensive* and people are mad t...</td>\n      <td>[a, statue, at, a, museum, that, no, seem, to,...</td>\n      <td>[the, statue, be, offensive, and, people, be, ...</td>\n      <td>[False, False, False, False, False, False, Fal...</td>\n      <td>[False, False, False, True, False, False, Fals...</td>\n      <td>[-1.0000000150474662e+30, 3.6017332077026367, ...</td>\n      <td>[-1.0000000150474662e+30, 3.75215744972229, -1...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# fname = './../.cache/dataset/esnli/test.pretransformed.parquet'\n",
    "\n",
    "fname = './../../RUNS/dataset/esnli/test.pretransformed.parquet'\n",
    "df = pd.read_parquet(fname)\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T21:23:55.090917Z",
     "start_time": "2023-10-04T21:23:54.859605Z"
    }
   },
   "id": "c48a1aa4befb13fb"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['id', 'premise', 'hypothesis', 'label', 'explanation',\n       'highlight_premise', 'highlight_hypothesis', 'tokens.norm.premise',\n       'tokens.norm.hypothesis', 'rationale.premise', 'rationale.hypothesis',\n       'heuristic.premise', 'heuristic.hypothesis'],\n      dtype='object')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T21:23:55.918869Z",
     "start_time": "2023-10-04T21:23:55.855390Z"
    }
   },
   "id": "339f2f38eb910f6d"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from data.transforms import SpacyTokenizerTransform\n",
    "\n",
    "import spacy\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "transform = SpacyTokenizerTransform(spacy_model)\n",
    "\n",
    "df['tokens.form.premise'] = transform(df['premise'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T21:24:58.961160Z",
     "start_time": "2023-10-04T21:24:40.084351Z"
    }
   },
   "id": "86cd09af8101c6bf"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df['tokens.form.hypothesis'] = transform(df['hypothesis'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T21:25:08.233773Z",
     "start_time": "2023-10-04T21:24:58.977052Z"
    }
   },
   "id": "f50c483fe5257e30"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df.to_parquet(fname, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T21:25:08.510145Z",
     "start_time": "2023-10-04T21:25:08.252275Z"
    }
   },
   "id": "59884fa787ba44ce"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "from data.transforms import SpacyTokenizerTransform\n",
    "\n",
    "import spacy\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "transform = SpacyTokenizerTransform(spacy_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T12:11:00.620151Z",
     "start_time": "2023-10-03T12:10:56.053695Z"
    }
   },
   "id": "5685d51e4cbb43d4"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "df['tokens.form'] = transform(df['text'].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T12:11:19.470721Z",
     "start_time": "2023-10-03T12:11:15.290878Z"
    }
   },
   "id": "6185a88ff8c66d6"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "premise = df['premise'].tolist()\n",
    "hypothesis = df['hypothesis'].tolist()\n",
    "premise_toks = transform(premise)\n",
    "hypothesis_toks = transform(hypothesis)\n",
    "\n",
    "df['tokens.form.premise'] = premise_toks\n",
    "df['tokens.form.hypothesis'] = hypothesis_toks"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T21:25:35.953810Z",
     "start_time": "2023-10-02T21:24:56.670600Z"
    }
   },
   "id": "d7b87337aaebce4d"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "'./../.cache/dataset/esnli/test.pretransformed.parquet'"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T21:26:08.217606Z",
     "start_time": "2023-10-02T21:26:08.161880Z"
    }
   },
   "id": "2921be86b4d3bb24"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  text  label  \\\n0    Out in Twinsburg for work and wasn't expecting...      1   \n1    Very slow. Never been in the drive at any othe...      0   \n2    Food is good, but service terrible. They have ...      0   \n3    Stopped by on a Sunday around 11am after a tri...      1   \n4    This place is horrible. They are very stingy w...      0   \n..                                                 ...    ...   \n295  Service and staff were very good. Topping sele...      1   \n296  Love it! Eaten here over 300 times in the last...      1   \n297  According to my friend, this local bar type pl...      1   \n298  I went here to get a snack before I went on th...      0   \n299  Always packed for lunch.  Probably because Pit...      0   \n\n                                            ham_html_0 human_label_0  \\\n0    <span>Out</span> <span>in</span> <span>Twinsbu...           yes   \n1    <span class=\"active\">Very</span> <span class=\"...            no   \n2    <span>Food</span> <span>is</span> <span class=...           idk   \n3    <span>Stopped</span> <span>by</span> <span>on<...           yes   \n4    <span>This</span> <span>place</span> <span>is<...            no   \n..                                                 ...           ...   \n295  <span class=\"active\">Service</span> <span>and<...           yes   \n296  <span class=\"active\">Love</span> <span>it!</sp...           yes   \n297  <span>According</span> <span>to</span> <span>m...           yes   \n298  <span>I</span> <span>went</span> <span>here</s...            no   \n299  <span>Always</span> <span>packed</span> <span>...            no   \n\n                                            ham_html_1 human_label_1  \\\n0    <span>Out</span> <span>in</span> <span>Twinsbu...           yes   \n1    <span>Very</span> <span class=\"active\">slow.</...            no   \n2    <span>Food</span> <span>is</span> <span>good,<...            no   \n3    <span>Stopped</span> <span>by</span> <span>on<...           yes   \n4    <span>This</span> <span>place</span> <span>is<...            no   \n..                                                 ...           ...   \n295  <span>Service</span> <span>and</span> <span>st...           yes   \n296  <span class=\"active\">Love</span> <span class=\"...           yes   \n297  <span>According</span> <span>to</span> <span>m...           yes   \n298  <span>I</span> <span>went</span> <span>here</s...            no   \n299  <span>Always</span> <span>packed</span> <span>...            no   \n\n                                            ham_html_2 human_label_2  \\\n0    <span>Out</span> <span>in</span> <span>Twinsbu...           yes   \n1    <span>Very</span> <span class=\"active\">slow.</...            no   \n2    <span>Food</span> <span>is</span> <span>good,<...            no   \n3    <span>Stopped</span> <span>by</span> <span>on<...           yes   \n4    <span>This</span> <span>place</span> <span>is<...            no   \n..                                                 ...           ...   \n295  <span>Service</span> <span>and</span> <span>st...           yes   \n296  <span class=\"active\">Love</span> <span class=\"...           yes   \n297  <span>According</span> <span>to</span> <span>m...           yes   \n298  <span>I</span> <span>went</span> <span>here</s...            no   \n299  <span class=\"active\">Always</span> <span class...            no   \n\n                         id  \\\n0      ham_part1(50words)_1   \n1      ham_part1(50words)_2   \n2      ham_part1(50words)_3   \n3      ham_part1(50words)_4   \n4      ham_part1(50words)_5   \n..                      ...   \n295  ham_part1(50words)_296   \n296  ham_part1(50words)_297   \n297  ham_part1(50words)_298   \n298  ham_part1(50words)_299   \n299  ham_part1(50words)_300   \n\n                                                 ham_0  \\\n0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2    [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4    [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n..                                                 ...   \n295  [1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, ...   \n296  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, ...   \n297  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n298  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n299  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                                 ham_1  \\\n0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2    [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4    [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n..                                                 ...   \n295  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, ...   \n296  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n297  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...   \n298  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n299  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                                 ham_2  \\\n0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4    [0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, ...   \n..                                                 ...   \n295  [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...   \n296  [1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, ...   \n297  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, ...   \n298  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n299  [1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n\n                                           tokens.norm  \\\n0    [out, in, twinsburg, for, work, and, be, not, ...   \n1    [very, slow, ., never, be, in, the, drive, at,...   \n2    [food, be, good, ,, but, service, terrible, .,...   \n3    [stop, by, on, a, sunday, around, 11, am, afte...   \n4    [this, place, be, horrible, ., they, be, very,...   \n..                                                 ...   \n295  [service, and, staff, be, very, good, ., toppi...   \n296  [love, it, !, eat, here, over, 300, time, in, ...   \n297  [accord, to, my, friend, ,, this, local, bar, ...   \n298  [i, go, here, to, get, a, snack, before, i, go...   \n299  [always, pack, for, lunch, ., probably, becaus...   \n\n                                           tokens.form  \\\n0    [Out, in, Twinsburg, for, work, and, was, n't,...   \n1    [Very, slow, ., Never, been, in, the, drive, a...   \n2    [Food, is, good, ,, but, service, terrible, .,...   \n3    [Stopped, by, on, a, Sunday, around, 11, am, a...   \n4    [This, place, is, horrible, ., They, are, very...   \n..                                                 ...   \n295  [Service, and, staff, were, very, good, ., Top...   \n296  [Love, it, !, Eaten, here, over, 300, times, i...   \n297  [According, to, my, friend, ,, this, local, ba...   \n298  [I, went, here, to, get, a, snack, before, I, ...   \n299  [Always, packed, for, lunch, ., Probably, beca...   \n\n                                                   ham  \\\n0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4    [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n..                                                 ...   \n295  [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...   \n296  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, ...   \n297  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...   \n298  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n299  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                                   cam  \\\n0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4    [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n..                                                 ...   \n295  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...   \n296  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n297  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n298  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n299  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                                   sam  \\\n0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2    [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4    [0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, ...   \n..                                                 ...   \n295  [1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, ...   \n296  [1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, ...   \n297  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, ...   \n298  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n299  [1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n\n                                             heuristic  \n0    [0.0, 0.0, 0.0, 0.0, 0.00042654533821490776, 0...  \n1    [0.0, 0.0023104539153307505, 0.0, 0.0, 0.0, 0....  \n2    [0.023922084384886078, 0.0, 0.0225002665908363...  \n3    [0.00039099989336366543, 0.0, 0.0, 0.0, 0.0, 0...  \n4    [0.0, 0.012405360253083567, 0.0, 0.00312799914...  \n..                                                 ...  \n295  [0.019869903671844453, 0.0, 0.0042654533821490...  \n296  [0.008495361319446913, 0.0, 0.0, 0.00252372658...  \n297  [0.0, 0.0, 0.0, 0.0003554544485124231, 0.0, 0....  \n298  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.5545444851242...  \n299  [0.0, 7.109088970248463e-05, 0.0, 0.0004265453...  \n\n[300 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>ham_html_0</th>\n      <th>human_label_0</th>\n      <th>ham_html_1</th>\n      <th>human_label_1</th>\n      <th>ham_html_2</th>\n      <th>human_label_2</th>\n      <th>id</th>\n      <th>ham_0</th>\n      <th>ham_1</th>\n      <th>ham_2</th>\n      <th>tokens.norm</th>\n      <th>tokens.form</th>\n      <th>ham</th>\n      <th>cam</th>\n      <th>sam</th>\n      <th>heuristic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Out in Twinsburg for work and wasn't expecting...</td>\n      <td>1</td>\n      <td>&lt;span&gt;Out&lt;/span&gt; &lt;span&gt;in&lt;/span&gt; &lt;span&gt;Twinsbu...</td>\n      <td>yes</td>\n      <td>&lt;span&gt;Out&lt;/span&gt; &lt;span&gt;in&lt;/span&gt; &lt;span&gt;Twinsbu...</td>\n      <td>yes</td>\n      <td>&lt;span&gt;Out&lt;/span&gt; &lt;span&gt;in&lt;/span&gt; &lt;span&gt;Twinsbu...</td>\n      <td>yes</td>\n      <td>ham_part1(50words)_1</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[out, in, twinsburg, for, work, and, be, not, ...</td>\n      <td>[Out, in, Twinsburg, for, work, and, was, n't,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.00042654533821490776, 0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Very slow. Never been in the drive at any othe...</td>\n      <td>0</td>\n      <td>&lt;span class=\"active\"&gt;Very&lt;/span&gt; &lt;span class=\"...</td>\n      <td>no</td>\n      <td>&lt;span&gt;Very&lt;/span&gt; &lt;span class=\"active\"&gt;slow.&lt;/...</td>\n      <td>no</td>\n      <td>&lt;span&gt;Very&lt;/span&gt; &lt;span class=\"active\"&gt;slow.&lt;/...</td>\n      <td>no</td>\n      <td>ham_part1(50words)_2</td>\n      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[very, slow, ., never, be, in, the, drive, at,...</td>\n      <td>[Very, slow, ., Never, been, in, the, drive, a...</td>\n      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0.0, 0.0023104539153307505, 0.0, 0.0, 0.0, 0....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Food is good, but service terrible. They have ...</td>\n      <td>0</td>\n      <td>&lt;span&gt;Food&lt;/span&gt; &lt;span&gt;is&lt;/span&gt; &lt;span class=...</td>\n      <td>idk</td>\n      <td>&lt;span&gt;Food&lt;/span&gt; &lt;span&gt;is&lt;/span&gt; &lt;span&gt;good,&lt;...</td>\n      <td>no</td>\n      <td>&lt;span&gt;Food&lt;/span&gt; &lt;span&gt;is&lt;/span&gt; &lt;span&gt;good,&lt;...</td>\n      <td>no</td>\n      <td>ham_part1(50words)_3</td>\n      <td>[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[food, be, good, ,, but, service, terrible, .,...</td>\n      <td>[Food, is, good, ,, but, service, terrible, .,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n      <td>[0.023922084384886078, 0.0, 0.0225002665908363...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Stopped by on a Sunday around 11am after a tri...</td>\n      <td>1</td>\n      <td>&lt;span&gt;Stopped&lt;/span&gt; &lt;span&gt;by&lt;/span&gt; &lt;span&gt;on&lt;...</td>\n      <td>yes</td>\n      <td>&lt;span&gt;Stopped&lt;/span&gt; &lt;span&gt;by&lt;/span&gt; &lt;span&gt;on&lt;...</td>\n      <td>yes</td>\n      <td>&lt;span&gt;Stopped&lt;/span&gt; &lt;span&gt;by&lt;/span&gt; &lt;span&gt;on&lt;...</td>\n      <td>yes</td>\n      <td>ham_part1(50words)_4</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[stop, by, on, a, sunday, around, 11, am, afte...</td>\n      <td>[Stopped, by, on, a, Sunday, around, 11, am, a...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0.00039099989336366543, 0.0, 0.0, 0.0, 0.0, 0...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>This place is horrible. They are very stingy w...</td>\n      <td>0</td>\n      <td>&lt;span&gt;This&lt;/span&gt; &lt;span&gt;place&lt;/span&gt; &lt;span&gt;is&lt;...</td>\n      <td>no</td>\n      <td>&lt;span&gt;This&lt;/span&gt; &lt;span&gt;place&lt;/span&gt; &lt;span&gt;is&lt;...</td>\n      <td>no</td>\n      <td>&lt;span&gt;This&lt;/span&gt; &lt;span&gt;place&lt;/span&gt; &lt;span&gt;is&lt;...</td>\n      <td>no</td>\n      <td>ham_part1(50words)_5</td>\n      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, ...</td>\n      <td>[this, place, be, horrible, ., they, be, very,...</td>\n      <td>[This, place, is, horrible, ., They, are, very...</td>\n      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, ...</td>\n      <td>[0.0, 0.012405360253083567, 0.0, 0.00312799914...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>Service and staff were very good. Topping sele...</td>\n      <td>1</td>\n      <td>&lt;span class=\"active\"&gt;Service&lt;/span&gt; &lt;span&gt;and&lt;...</td>\n      <td>yes</td>\n      <td>&lt;span&gt;Service&lt;/span&gt; &lt;span&gt;and&lt;/span&gt; &lt;span&gt;st...</td>\n      <td>yes</td>\n      <td>&lt;span&gt;Service&lt;/span&gt; &lt;span&gt;and&lt;/span&gt; &lt;span&gt;st...</td>\n      <td>yes</td>\n      <td>ham_part1(50words)_296</td>\n      <td>[1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, ...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, ...</td>\n      <td>[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n      <td>[service, and, staff, be, very, good, ., toppi...</td>\n      <td>[Service, and, staff, were, very, good, ., Top...</td>\n      <td>[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n      <td>[1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, ...</td>\n      <td>[0.019869903671844453, 0.0, 0.0042654533821490...</td>\n    </tr>\n    <tr>\n      <th>296</th>\n      <td>Love it! Eaten here over 300 times in the last...</td>\n      <td>1</td>\n      <td>&lt;span class=\"active\"&gt;Love&lt;/span&gt; &lt;span&gt;it!&lt;/sp...</td>\n      <td>yes</td>\n      <td>&lt;span class=\"active\"&gt;Love&lt;/span&gt; &lt;span class=\"...</td>\n      <td>yes</td>\n      <td>&lt;span class=\"active\"&gt;Love&lt;/span&gt; &lt;span class=\"...</td>\n      <td>yes</td>\n      <td>ham_part1(50words)_297</td>\n      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, ...</td>\n      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, ...</td>\n      <td>[love, it, !, eat, here, over, 300, time, in, ...</td>\n      <td>[Love, it, !, Eaten, here, over, 300, times, i...</td>\n      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, ...</td>\n      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, ...</td>\n      <td>[0.008495361319446913, 0.0, 0.0, 0.00252372658...</td>\n    </tr>\n    <tr>\n      <th>297</th>\n      <td>According to my friend, this local bar type pl...</td>\n      <td>1</td>\n      <td>&lt;span&gt;According&lt;/span&gt; &lt;span&gt;to&lt;/span&gt; &lt;span&gt;m...</td>\n      <td>yes</td>\n      <td>&lt;span&gt;According&lt;/span&gt; &lt;span&gt;to&lt;/span&gt; &lt;span&gt;m...</td>\n      <td>yes</td>\n      <td>&lt;span&gt;According&lt;/span&gt; &lt;span&gt;to&lt;/span&gt; &lt;span&gt;m...</td>\n      <td>yes</td>\n      <td>ham_part1(50words)_298</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, ...</td>\n      <td>[accord, to, my, friend, ,, this, local, bar, ...</td>\n      <td>[According, to, my, friend, ,, this, local, ba...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0003554544485124231, 0.0, 0....</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>I went here to get a snack before I went on th...</td>\n      <td>0</td>\n      <td>&lt;span&gt;I&lt;/span&gt; &lt;span&gt;went&lt;/span&gt; &lt;span&gt;here&lt;/s...</td>\n      <td>no</td>\n      <td>&lt;span&gt;I&lt;/span&gt; &lt;span&gt;went&lt;/span&gt; &lt;span&gt;here&lt;/s...</td>\n      <td>no</td>\n      <td>&lt;span&gt;I&lt;/span&gt; &lt;span&gt;went&lt;/span&gt; &lt;span&gt;here&lt;/s...</td>\n      <td>no</td>\n      <td>ham_part1(50words)_299</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[i, go, here, to, get, a, snack, before, i, go...</td>\n      <td>[I, went, here, to, get, a, snack, before, I, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.5545444851242...</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>Always packed for lunch.  Probably because Pit...</td>\n      <td>0</td>\n      <td>&lt;span&gt;Always&lt;/span&gt; &lt;span&gt;packed&lt;/span&gt; &lt;span&gt;...</td>\n      <td>no</td>\n      <td>&lt;span&gt;Always&lt;/span&gt; &lt;span&gt;packed&lt;/span&gt; &lt;span&gt;...</td>\n      <td>no</td>\n      <td>&lt;span class=\"active\"&gt;Always&lt;/span&gt; &lt;span class...</td>\n      <td>no</td>\n      <td>ham_part1(50words)_300</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n      <td>[always, pack, for, lunch, ., probably, becaus...</td>\n      <td>[Always, packed, for, lunch, ., Probably, beca...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n      <td>[0.0, 7.109088970248463e-05, 0.0, 0.0004265453...</td>\n    </tr>\n  </tbody>\n</table>\n<p>300 rows × 18 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# fname = './../.cache/dataset/esnli/test.pretransformed.parquet'\n",
    "\n",
    "fname = './../../RUNS/dataset_/yelp-hat/yelp50.pretokenized_lower_lemma.parquet'\n",
    "df = pd.read_parquet(fname)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T14:57:25.914626Z",
     "start_time": "2023-10-04T14:57:25.083970Z"
    }
   },
   "id": "48276442ca61f7e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_CACHE = '/home/dunguyen/RUNS/dataset_'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f15f9c075d0f71f"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-10-2023 15:52:07 | \u001B[32;1m   DEBUG\u001B[0m \u001B[1m \u001B[4m dataset.py:download_format_dataset:82 \u001B[0m \u001B[32;1mCorrectly handle part7.csv\u001B[0m\n",
      "04-10-2023 15:52:08 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m dataset.py:download_format_dataset:110 \u001B[0m \u001B[34mSave yelp subset at: /home/dunguyen/RUNS/dataset_/yelp-hat/yelp200.parquet\u001B[0m\n",
      "04-10-2023 15:52:08 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m dataset.py:download_format_dataset:110 \u001B[0m \u001B[34mSave yelp subset at: /home/dunguyen/RUNS/dataset_/yelp-hat/yelp50.parquet\u001B[0m\n",
      "04-10-2023 15:52:08 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m dataset.py:download_format_dataset:110 \u001B[0m \u001B[34mSave yelp subset at: /home/dunguyen/RUNS/dataset_/yelp-hat/yelp100.parquet\u001B[0m\n",
      "04-10-2023 15:52:08 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m dataset.py:download_format_dataset:116 \u001B[0m \u001B[34mSave clean dataset at /home/dunguyen/RUNS/dataset_/yelp-hat/yelp.parquet\u001B[0m\n",
      "04-10-2023 15:52:08 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m dataset.py:download_format_dataset:123 \u001B[0m \u001B[34mSave training set at /home/dunguyen/RUNS/dataset_/yelp-hat/train.parquet\u001B[0m\n",
      "04-10-2023 15:52:08 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m dataset.py:download_format_dataset:125 \u001B[0m \u001B[34mSave validation dataset at /home/dunguyen/RUNS/dataset_/yelp-hat/val.parquet\u001B[0m\n",
      "04-10-2023 15:52:08 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m yelp_hat_module.py:prepare_data:89 \u001B[0m \u001B[34mLoaded vocab at /home/dunguyen/RUNS/dataset_/yelp-hat/vocab_lemma_lower.pt\u001B[0m\n",
      "04-10-2023 15:52:08 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m yelp_hat_module.py:prepare_data:91 \u001B[0m \u001B[34mVocab size: 7465\u001B[0m\n",
      "04-10-2023 15:52:08 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m dataset.py:__init__:186 \u001B[0m \u001B[34mLoad dataset from /home/dunguyen/RUNS/dataset_/yelp-hat/train.parquet\u001B[0m\n",
      "04-10-2023 15:55:15 | \u001B[33m WARNING\u001B[0m \u001B[1m \u001B[4m spacy_pretok_dataset.py:_reformat_dataframe:34 \u001B[0m \u001B[33mDrop 8 samples because HAMs are not compatibles\u001B[0m\n",
      "04-10-2023 15:55:15 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m transforms.py:_token_frequency:65 \u001B[0m \u001B[34mLoad annotation frequency from /home/dunguyen/RUNS/dataset_/yelp-hat/annotation_lexical_frequency.json\u001B[0m\n",
      "04-10-2023 15:55:30 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m spacy_pretok_dataset.py:__init__:79 \u001B[0m \u001B[34mSave yelp subset train at: /home/dunguyen/RUNS/dataset_/yelp-hat/train.pretokenized_lower_lemma.parquet\u001B[0m\n",
      "04-10-2023 15:55:30 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m dataset.py:__init__:186 \u001B[0m \u001B[34mLoad dataset from /home/dunguyen/RUNS/dataset_/yelp-hat/val.parquet\u001B[0m\n",
      "04-10-2023 15:56:51 | \u001B[33m WARNING\u001B[0m \u001B[1m \u001B[4m spacy_pretok_dataset.py:_reformat_dataframe:34 \u001B[0m \u001B[33mDrop 4 samples because HAMs are not compatibles\u001B[0m\n",
      "04-10-2023 15:56:52 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m transforms.py:_token_frequency:65 \u001B[0m \u001B[34mLoad annotation frequency from /home/dunguyen/RUNS/dataset_/yelp-hat/annotation_lexical_frequency.json\u001B[0m\n",
      "04-10-2023 15:56:58 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m spacy_pretok_dataset.py:__init__:79 \u001B[0m \u001B[34mSave yelp subset val at: /home/dunguyen/RUNS/dataset_/yelp-hat/val.pretokenized_lower_lemma.parquet\u001B[0m\n",
      "04-10-2023 15:56:58 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m dataset.py:__init__:186 \u001B[0m \u001B[34mLoad dataset from /home/dunguyen/RUNS/dataset_/yelp-hat/yelp50.parquet\u001B[0m\n",
      "04-10-2023 15:57:18 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m transforms.py:_token_frequency:65 \u001B[0m \u001B[34mLoad annotation frequency from /home/dunguyen/RUNS/dataset_/yelp-hat/annotation_lexical_frequency.json\u001B[0m\n",
      "04-10-2023 15:57:19 | \u001B[34m    INFO\u001B[0m \u001B[1m \u001B[4m spacy_pretok_dataset.py:__init__:79 \u001B[0m \u001B[34mSave yelp subset yelp50 at: /home/dunguyen/RUNS/dataset_/yelp-hat/yelp50.pretokenized_lower_lemma.parquet\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "from data_module.yelp_hat_module import YelpHat50DM\n",
    "yelphat_dm = YelpHat50DM(cache_path=DATA_CACHE, batch_size=16)\n",
    "yelphat_dm.prepare_data()\n",
    "yelphat_dm.setup()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T13:57:19.960492Z",
     "start_time": "2023-10-04T13:52:04.968600Z"
    }
   },
   "id": "6d2ecdb65bbe8ed4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d93e39fd3a210cf0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
