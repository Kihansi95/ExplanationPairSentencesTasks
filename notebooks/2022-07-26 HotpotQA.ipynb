{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Study on HotpotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./../src\")\n",
    "\n",
    "cache_path = path.join(os.getcwd(), '..', '.cache')\n",
    "tmp_path = path.join('.cache', '2022-07-26')\n",
    "os.makedirs(tmp_path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset hotpot_qa (/Users/dunguyen/Projects/explanation_on_pair_sequences_task/notebooks/../.cache/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_set= load_dataset('hotpot_qa', 'distractor', split='train', cache_dir=cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 90447 entries, 0 to 90446\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   id                90447 non-null  object\n",
      " 1   question          90447 non-null  object\n",
      " 2   answer            90447 non-null  object\n",
      " 3   type              90447 non-null  object\n",
      " 4   level             90447 non-null  object\n",
      " 5   supporting_facts  90447 non-null  object\n",
      " 6   context           90447 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train = train_set.to_pandas()\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which magazine was started first Arthur's Magazine or First for Women?\n",
      "['which', 'magazine', 'be', 'start', 'first', 'Arthur', \"'s\", 'Magazine', 'or', 'first', 'for', 'Women', '?']\n",
      "The Oberoi family is part of a hotel company that has a head office in what city?\n",
      "['the', 'oberoi', 'family', 'be', 'part', 'of', 'a', 'hotel', 'company', 'that', 'have', 'a', 'head', 'office', 'in', 'what', 'city', '?']\n",
      "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
      "['musician', 'and', 'satirist', 'Allie', 'Goertz', 'write', 'a', 'song', 'about', 'the', '\"', 'the', 'Simpsons', '\"', 'character', 'Milhouse', ',', 'who', 'Matt', 'Groening', 'name', 'after', 'who', '?']\n",
      " What nationality was James Henry Miller's wife?\n",
      "[' ', 'what', 'nationality', 'be', 'James', 'Henry', 'Miller', \"'s\", 'wife', '?']\n",
      "Cadmium Chloride is slightly soluble in this chemical, it is also called what?\n",
      "['Cadmium', 'Chloride', 'be', 'slightly', 'soluble', 'in', 'this', 'chemical', ',', 'it', 'be', 'also', 'call', 'what', '?']\n",
      "Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?\n",
      "['which', 'tennis', 'player', 'win', 'more', 'Grand', 'Slam', 'title', ',', 'Henri', 'Leconte', 'or', 'Jonathan', 'Stark', '?']\n",
      "Which genus of moth in the world's seventh-largest country contains only one species?\n",
      "['which', 'genus', 'of', 'moth', 'in', 'the', 'world', \"'s\", 'seventh', '-', 'large', 'country', 'contain', 'only', 'one', 'specie', '?']\n",
      "Who was once considered the best kick boxer in the world, however he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring.\n",
      "['who', 'be', 'once', 'consider', 'the', 'good', 'kick', 'boxer', 'in', 'the', 'world', ',', 'however', 'he', 'have', 'be', 'involve', 'in', 'a', 'number', 'of', 'controversy', 'relate', 'to', 'his', '\"', 'unsportsmanlike', 'conduct', '\"', 'in', 'the', 'sport', 'and', 'crime', 'of', 'violence', 'outside', 'of', 'the', 'ring', '.']\n",
      "The Dutch-Belgian television series that \"House of Anubis\" was based on first aired in what year?\n",
      "['the', 'Dutch', '-', 'belgian', 'television', 'series', 'that', '\"', 'House', 'of', 'Anubis', '\"', 'be', 'base', 'on', 'first', 'air', 'in', 'what', 'year', '?']\n",
      "What is the length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged?\n",
      "['what', 'be', 'the', 'length', 'of', 'the', 'track', 'where', 'the', '2013', 'Liqui', 'Moly', 'Bathurst', '12', 'Hour', 'be', 'stage', '?']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "for doc in spacy_model.pipe(df_train['question'][:10]):\n",
    "    print(doc)\n",
    "    print([tk.lemma_ for tk in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which magazine was started first Arthur's Magazine or First for Women?\n",
      "The Oberoi family is part of a hotel company that has a head office in what city?\n",
      "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
      " What nationality was James Henry Miller's wife?\n",
      "Cadmium Chloride is slightly soluble in this chemical, it is also called what?\n",
      "==============================\n",
      "gpt2 : ['Which', 'Ġmagazine', 'Ġwas', 'Ġstarted', 'Ġfirst', 'ĠArthur', \"'s\", 'ĠMagazine', 'Ġor', 'ĠFirst', 'Ġfor', 'ĠWomen', '?']\n",
      "gpt2 : ['The', 'ĠOber', 'oi', 'Ġfamily', 'Ġis', 'Ġpart', 'Ġof', 'Ġa', 'Ġhotel', 'Ġcompany', 'Ġthat', 'Ġhas', 'Ġa', 'Ġhead', 'Ġoffice', 'Ġin', 'Ġwhat', 'Ġcity', '?']\n",
      "gpt2 : ['Mus', 'ician', 'Ġand', 'Ġsatir', 'ist', 'ĠAll', 'ie', 'ĠGo', 'ert', 'z', 'Ġwrote', 'Ġa', 'Ġsong', 'Ġabout', 'Ġthe', 'Ġ\"', 'The', 'ĠSimpsons', '\"', 'Ġcharacter', 'ĠMil', 'house', ',', 'Ġwho', 'ĠMatt', 'ĠGro', 'ening', 'Ġnamed', 'Ġafter', 'Ġwho', '?']\n",
      "gpt2 : ['ĠWhat', 'Ġnationality', 'Ġwas', 'ĠJames', 'ĠHenry', 'ĠMiller', \"'s\", 'Ġwife', '?']\n",
      "gpt2 : ['C', 'ad', 'm', 'ium', 'ĠCh', 'lor', 'ide', 'Ġis', 'Ġslightly', 'Ġsoluble', 'Ġin', 'Ġthis', 'Ġchemical', ',', 'Ġit', 'Ġis', 'Ġalso', 'Ġcalled', 'Ġwhat', '?']\n",
      "==============================\n",
      "distilbert-base-uncased : ['which', 'magazine', 'was', 'started', 'first', 'arthur', \"'\", 's', 'magazine', 'or', 'first', 'for', 'women', '?']\n",
      "distilbert-base-uncased : ['the', 'obe', '##roi', 'family', 'is', 'part', 'of', 'a', 'hotel', 'company', 'that', 'has', 'a', 'head', 'office', 'in', 'what', 'city', '?']\n",
      "distilbert-base-uncased : ['musician', 'and', 'sat', '##iri', '##st', 'allie', 'go', '##ert', '##z', 'wrote', 'a', 'song', 'about', 'the', '\"', 'the', 'simpsons', '\"', 'character', 'mil', '##house', ',', 'who', 'matt', 'gr', '##oe', '##ning', 'named', 'after', 'who', '?']\n",
      "distilbert-base-uncased : ['what', 'nationality', 'was', 'james', 'henry', 'miller', \"'\", 's', 'wife', '?']\n",
      "distilbert-base-uncased : ['cad', '##mium', 'chloride', 'is', 'slightly', 'soluble', 'in', 'this', 'chemical', ',', 'it', 'is', 'also', 'called', 'what', '?']\n",
      "==============================\n",
      "bert-base-uncased : ['which', 'magazine', 'was', 'started', 'first', 'arthur', \"'\", 's', 'magazine', 'or', 'first', 'for', 'women', '?']\n",
      "bert-base-uncased : ['the', 'obe', '##roi', 'family', 'is', 'part', 'of', 'a', 'hotel', 'company', 'that', 'has', 'a', 'head', 'office', 'in', 'what', 'city', '?']\n",
      "bert-base-uncased : ['musician', 'and', 'sat', '##iri', '##st', 'allie', 'go', '##ert', '##z', 'wrote', 'a', 'song', 'about', 'the', '\"', 'the', 'simpsons', '\"', 'character', 'mil', '##house', ',', 'who', 'matt', 'gr', '##oe', '##ning', 'named', 'after', 'who', '?']\n",
      "bert-base-uncased : ['what', 'nationality', 'was', 'james', 'henry', 'miller', \"'\", 's', 'wife', '?']\n",
      "bert-base-uncased : ['cad', '##mium', 'chloride', 'is', 'slightly', 'soluble', 'in', 'this', 'chemical', ',', 'it', 'is', 'also', 'called', 'what', '?']\n",
      "==============================\n",
      "dslim/bert-base-NER : ['Which', 'magazine', 'was', 'started', 'first', 'Arthur', \"'\", 's', 'Magazine', 'or', 'First', 'for', 'Women', '?']\n",
      "dslim/bert-base-NER : ['The', 'O', '##ber', '##oi', 'family', 'is', 'part', 'of', 'a', 'hotel', 'company', 'that', 'has', 'a', 'head', 'office', 'in', 'what', 'city', '?']\n",
      "dslim/bert-base-NER : ['Music', '##ian', 'and', 'sat', '##iri', '##st', 'Allie', 'Go', '##ert', '##z', 'wrote', 'a', 'song', 'about', 'the', '\"', 'The', 'Simpsons', '\"', 'character', 'Mi', '##l', '##house', ',', 'who', 'Matt', 'G', '##ro', '##ening', 'named', 'after', 'who', '?']\n",
      "dslim/bert-base-NER : ['What', 'nationality', 'was', 'James', 'Henry', 'Miller', \"'\", 's', 'wife', '?']\n",
      "dslim/bert-base-NER : ['C', '##ad', '##mium', 'Ch', '##lor', '##ide', 'is', 'slightly', 'soluble', 'in', 'this', 'chemical', ',', 'it', 'is', 'also', 'called', 'what', '?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "pretrained_tokenizer = [\n",
    "    'gpt2', 'distilbert-base-uncased', 'bert-base-uncased', 'dslim/bert-base-NER'\n",
    "]\n",
    "\n",
    "tokenized_question = dict()\n",
    "\n",
    "for question in df_train['question'][:5].tolist():\n",
    "    print(question)\n",
    "\n",
    "for pretrained in pretrained_tokenizer:\n",
    "    print('='*30)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained, cache_dir=cache_path)\n",
    "    for idx_question in range(5):\n",
    "        print(pretrained,':', tokenizer.tokenize(df_train['question'][idx_question]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Which magazine was started first Arthur's Magazine or First for Women?\",\n",
       " 'The Oberoi family is part of a hotel company that has a head office in what city?',\n",
       " 'Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?',\n",
       " \" What nationality was James Henry Miller's wife?\",\n",
       " 'Cadmium Chloride is slightly soluble in this chemical, it is also called what?',\n",
       " 'Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?',\n",
       " \"Which genus of moth in the world's seventh-largest country contains only one species?\",\n",
       " 'Who was once considered the best kick boxer in the world, however he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring.',\n",
       " 'The Dutch-Belgian television series that \"House of Anubis\" was based on first aired in what year?',\n",
       " 'What is the length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['question'][:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\", cache_dir=cache_path)\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained('dslim/bert-base-NER', cache_dir=cache_path)\n",
    "\n",
    "inputs = ner_tokenizer(df_train['question'][0], return_tensors=\"pt\")\n",
    "outputs = ner_model(**inputs).logits\n",
    "predictions = torch.argmax(outputs, dim=2).squeeze(0)\n",
    "labels = [ner_model.config.id2label[p.item()] for p in predictions]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Check % of Named Entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Exemple des entités nommés dans les 10 premiers phrases dans `question`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Arthur's Magazine\", 'First for Women']\n",
      "['Oberoi']\n",
      "['Musician', 'Allie Goertz', 'the \"The Simpsons', 'Milhouse', 'Matt Groening']\n",
      "[\"James Henry Miller's\"]\n",
      "['Cadmium Chloride']\n",
      "['Grand Slam', 'Jonathan Stark']\n",
      "['seventh', 'only one']\n",
      "[]\n",
      "['Dutch', 'House of Anubis', 'what year']\n",
      "['2013', '12 Hour']\n"
     ]
    }
   ],
   "source": [
    "docs = spacy_model.pipe(df_train['question'][:10])\n",
    "for d in docs:\n",
    "    print([ent.text for ent in d.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Une entité est-elle compté comme un token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longueur de span d'entité ; nombre de token par phrase\n",
      "[3, 3] 13\n",
      "[1] 18\n",
      "[1, 2, 4, 1, 2] 24\n",
      "[4] 10\n",
      "[2] 15\n",
      "[2, 2] 15\n",
      "[1, 2] 17\n",
      "[] 41\n",
      "[1, 3, 2] 21\n",
      "[1, 2] 18\n"
     ]
    }
   ],
   "source": [
    "docs = spacy_model.pipe(df_train['question'][:10])\n",
    "\n",
    "print('Longueur de span d\\'entité ; nombre de token par phrase')\n",
    "entity_span = list()\n",
    "\n",
    "for d in docs:\n",
    "    distances = [ent.end - ent.start for ent in d.ents]\n",
    "    entity_span.append(distances)\n",
    "    print(distances, len(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Pourcentage de token couvert par question si nous comptons en tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset hotpot_qa (/Users/dunguyen/Projects/explanation_on_pair_sequences_task/notebooks/../.cache/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b2538e098442db9f542bc243cede4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5b6e8917de46fa9b16de9df79ef35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/298331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset hotpot_qa (/Users/dunguyen/Projects/explanation_on_pair_sequences_task/notebooks/../.cache/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef41efa51374b81bfa94553a4a086f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5a3a201bb145c2b13af0a102311217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save stats in .cache/2022-07-26/stats_entity.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>train</th><th>%Token_in_entity</th><th>%Entity_as_token</th></tr><tr><td>question</td><td>23.91%</td><td>14.0%</td></tr><tr><td>context</td><td>25.8%</td><td>14.89%</td></tr><tr><td>total</td><td>25.77%</td><td>14.87%</td></tr><tr><th>validation</th><th>%Token_in_entity</th><th>%Entity_as_token</th></tr><tr><td>question</td><td>23.21%</td><td>13.36%</td></tr><tr><td>context</td><td>25.64%</td><td>14.79%</td></tr><tr><td>total</td><td>25.6%</td><td>14.76%</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "stats_path = path.join(tmp_path, 'stats_entity.json')\n",
    "if path.exists(stats_path):\n",
    "    with open(stats_path, 'r') as f:\n",
    "        stats = json.load(f)\n",
    "    print(f'Load stats in {stats_path}')\n",
    "\n",
    "else:\n",
    "    stats = dict()\n",
    "    for split in ['train', 'validation']:\n",
    "        dataset = load_dataset('hotpot_qa', 'distractor', split=split, cache_dir=cache_path)\n",
    "        dataset = dataset.to_pandas()\n",
    "        dataset = dataset[:30000].reset_index()\n",
    "        \n",
    "        stats[split] = {'question': dict(), 'context': dict()}\n",
    "\n",
    "\n",
    "        n_token_entity = 0\n",
    "        n_token_all = 0\n",
    "        n_entity_ent1tok = 0\n",
    "        n_token_ent1tok = 0\n",
    "\n",
    "        # Stats for Question\n",
    "        docs = spacy_model.pipe(dataset['question'])\n",
    "        for d in tqdm(docs, total=len(dataset['question'])):\n",
    "            length_entities = [ent.end - ent.start for ent in d.ents]\n",
    "            n_token_entity += sum(length_entities)\n",
    "\n",
    "            n_token_all += len(d)\n",
    "\n",
    "            n_entity_ent1tok += len(d.ents)\n",
    "\n",
    "            n_token_ent1tok += len(d) - sum(length_entities) + len(d.ents)\n",
    "\n",
    "        stats[split]['question'] = {\n",
    "            'n_token_entity': n_token_entity, \n",
    "            'n_token_all': n_token_all,\n",
    "            'n_entity_ent1tok': n_entity_ent1tok,\n",
    "            'n_token_ent1tok': n_token_ent1tok\n",
    "        }\n",
    "\n",
    "        # Stats for Context\n",
    "        contexts = []\n",
    "        for context in dataset['context']:\n",
    "            contexts += [' '.join(c) for c in context['sentences']] \n",
    "\n",
    "        n_token_entity = 0\n",
    "        n_token_all = 0\n",
    "        n_entity_ent1tok = 0\n",
    "        n_token_ent1tok = 0\n",
    "        docs = spacy_model.pipe(contexts)\n",
    "        for d in tqdm(docs, total=len(contexts)):\n",
    "            length_entities = [ent.end - ent.start for ent in d.ents]\n",
    "            n_token_entity += sum(length_entities)\n",
    "\n",
    "            n_token_all += len(d)\n",
    "\n",
    "            n_entity_ent1tok += len(d.ents)\n",
    "\n",
    "            n_token_ent1tok += len(d) - sum(length_entities) + len(d.ents)\n",
    "\n",
    "        stats[split]['context'] = {\n",
    "            'n_token_entity': n_token_entity, \n",
    "            'n_token_all': n_token_all,\n",
    "            'n_entity_ent1tok': n_entity_ent1tok,\n",
    "            'n_token_ent1tok': n_token_ent1tok\n",
    "        }\n",
    "        \n",
    "        stats[split]['total'] = {key: sum([stats[split][part][key] for part in ['question', 'context']]) for key in stats[split]['context'] }\n",
    "\n",
    "    with open(stats_path, 'w') as f:\n",
    "        json.dump(stats, f)\n",
    "    print(f'Save stats in {stats_path}')\n",
    "    \n",
    "html = '<table>'\n",
    "\n",
    "for split, split_stat in stats.items():\n",
    "\n",
    "    html += f'<tr><th>{split}</th><th>%Token_in_entity</th><th>%Entity_as_token</th></tr>'\n",
    "\n",
    "    for k, v in split_stat.items():\n",
    "        n_token_entity = v['n_token_entity']\n",
    "        n_token_all = v['n_token_all']\n",
    "        n_entity_ent1tok = v['n_entity_ent1tok']\n",
    "        n_token_ent1tok = v['n_token_ent1tok']\n",
    "        html += f'<tr><td>{k}</td><td>{round(n_token_entity*100/n_token_all, 2)}%</td><td>{round(n_entity_ent1tok*100/n_token_ent1tok, 2)}%</td></tr>'\n",
    "\n",
    "html += '</table>'\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare HotpotQA dataset in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset hotpot_qa (/Users/dunguyen/Projects/explanation_on_pair_sequences_task/notebooks/../.cache/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5)\n"
     ]
    }
   ],
   "source": [
    "training_set = load_dataset('hotpot_qa', 'distractor', split='train', cache_dir=cache_path)\n",
    "training_set = training_set.train_test_split(test_size=0.3, shuffle=False)\n",
    "train_set = training_set['train']\n",
    "val_set = training_set['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for t in training_set:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5d07d55e2e49559b2f6882cecf1d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "id sent not valid in [title=7,sent=2], id=5a7b23ca554299042af8f703\n",
      "id sent not valid in [title=6,sent=20], id=5abed6d45542990832d3a0ef\n",
      "id sent not valid in [title=9,sent=3], id=5ab6b2fb5542995eadef0060\n",
      "id sent not valid in [title=9,sent=2], id=5ae0e2df5542990adbacf6b1\n",
      "id sent not valid in [title=1,sent=4], id=5a8d6138554299585d9e37c7\n",
      "id sent not valid in [title=3,sent=52], id=5ab740165542992aa3b8c7fa\n",
      "id sent not valid in [title=8,sent=2], id=5ab2f812554299545a2cfaee\n",
      "id sent not valid in [title=3,sent=52], id=5ae7e8ef5542994a481bbe05\n",
      "id sent not valid in [title=4,sent=4], id=5ab273ee5542997061209606\n",
      "id sent not valid in [title=4,sent=2], id=5a84517355429933447460d5\n",
      "id sent not valid in [title=3,sent=20], id=5a7e5b2455429934daa2fc10\n",
      "id sent not valid in [title=7,sent=40], id=5a846921554299123d8c2243\n",
      "id sent not valid in [title=6,sent=2], id=5add66475542992200553af1\n",
      "id sent not valid in [title=5,sent=2], id=5a847c91554299123d8c2268\n",
      "id sent not valid in [title=5,sent=30], id=5a7b629555429927d897bfa4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>facts</th>\n",
       "      <th>support</th>\n",
       "      <th>answer</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>Radio City is India's first private FM radio s...</td>\n",
       "      <td>[False, False, False, False, False, False, False]</td>\n",
       "      <td>False</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>Football in Albania existed before the Albania...</td>\n",
       "      <td>[False, False, False, False]</td>\n",
       "      <td>False</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>Echosmith is an American, Corporate indie pop ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>False</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>Women's colleges in the Southern United States...</td>\n",
       "      <td>[False, False, False, False]</td>\n",
       "      <td>False</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>The First Arthur County Courthouse and Jail, w...</td>\n",
       "      <td>[False]</td>\n",
       "      <td>False</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629571</th>\n",
       "      <td>Who was the producer of the series of politica...</td>\n",
       "      <td>The Angelina Jolie trapdoor spider (\"Aptostich...</td>\n",
       "      <td>[False, False, False]</td>\n",
       "      <td>False</td>\n",
       "      <td>Jerry Bruckheimer</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629572</th>\n",
       "      <td>Who was the producer of the series of politica...</td>\n",
       "      <td>Tiffany Claus (born July 14, 1980) is an Ameri...</td>\n",
       "      <td>[False, False, False, False]</td>\n",
       "      <td>False</td>\n",
       "      <td>Jerry Bruckheimer</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629573</th>\n",
       "      <td>Who was the producer of the series of politica...</td>\n",
       "      <td>Aptostichus miwok is a species of spider in th...</td>\n",
       "      <td>[False, False]</td>\n",
       "      <td>False</td>\n",
       "      <td>Jerry Bruckheimer</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629574</th>\n",
       "      <td>Who was the producer of the series of politica...</td>\n",
       "      <td>James Haven (born James Haven Voight; May 11, ...</td>\n",
       "      <td>[False, False]</td>\n",
       "      <td>False</td>\n",
       "      <td>Jerry Bruckheimer</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629575</th>\n",
       "      <td>Who was the producer of the series of politica...</td>\n",
       "      <td>Jolie is a female given name of French origin ...</td>\n",
       "      <td>[False, False, False, False]</td>\n",
       "      <td>False</td>\n",
       "      <td>Jerry Bruckheimer</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>629576 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  \\\n",
       "0       Which magazine was started first Arthur's Maga...   \n",
       "1       Which magazine was started first Arthur's Maga...   \n",
       "2       Which magazine was started first Arthur's Maga...   \n",
       "3       Which magazine was started first Arthur's Maga...   \n",
       "4       Which magazine was started first Arthur's Maga...   \n",
       "...                                                   ...   \n",
       "629571  Who was the producer of the series of politica...   \n",
       "629572  Who was the producer of the series of politica...   \n",
       "629573  Who was the producer of the series of politica...   \n",
       "629574  Who was the producer of the series of politica...   \n",
       "629575  Who was the producer of the series of politica...   \n",
       "\n",
       "                                                  context  \\\n",
       "0       Radio City is India's first private FM radio s...   \n",
       "1       Football in Albania existed before the Albania...   \n",
       "2       Echosmith is an American, Corporate indie pop ...   \n",
       "3       Women's colleges in the Southern United States...   \n",
       "4       The First Arthur County Courthouse and Jail, w...   \n",
       "...                                                   ...   \n",
       "629571  The Angelina Jolie trapdoor spider (\"Aptostich...   \n",
       "629572  Tiffany Claus (born July 14, 1980) is an Ameri...   \n",
       "629573  Aptostichus miwok is a species of spider in th...   \n",
       "629574  James Haven (born James Haven Voight; May 11, ...   \n",
       "629575  Jolie is a female given name of French origin ...   \n",
       "\n",
       "                                                    facts  support  \\\n",
       "0       [False, False, False, False, False, False, False]    False   \n",
       "1                            [False, False, False, False]    False   \n",
       "2       [False, False, False, False, False, False, Fal...    False   \n",
       "3                            [False, False, False, False]    False   \n",
       "4                                                 [False]    False   \n",
       "...                                                   ...      ...   \n",
       "629571                              [False, False, False]    False   \n",
       "629572                       [False, False, False, False]    False   \n",
       "629573                                     [False, False]    False   \n",
       "629574                                     [False, False]    False   \n",
       "629575                       [False, False, False, False]    False   \n",
       "\n",
       "                   answer   level  \n",
       "0       Arthur's Magazine  medium  \n",
       "1       Arthur's Magazine  medium  \n",
       "2       Arthur's Magazine  medium  \n",
       "3       Arthur's Magazine  medium  \n",
       "4       Arthur's Magazine  medium  \n",
       "...                   ...     ...  \n",
       "629571  Jerry Bruckheimer  medium  \n",
       "629572  Jerry Bruckheimer  medium  \n",
       "629573  Jerry Bruckheimer  medium  \n",
       "629574  Jerry Bruckheimer  medium  \n",
       "629575  Jerry Bruckheimer  medium  \n",
       "\n",
       "[629576 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dict = {'question': list(), \n",
    "             'context': list(),\n",
    "             'facts': list(), \n",
    "             'support': list(),\n",
    "            'answer': list(),\n",
    "            'level': list()}\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for row in tqdm(train_set, total=len(train_set)):\n",
    "    \n",
    "    _id = row['id']\n",
    "    question = row['question']\n",
    "    \n",
    "    # find ids of context:\n",
    "    facts = row['supporting_facts']\n",
    "    context = row['context']\n",
    "    context_length = [len(c) for c in context['sentences']]\n",
    "    context_text = [' '.join(c) for c in context['sentences']]\n",
    "    facts_mask = [[False]*cl for cl in context_length]\n",
    "    \n",
    "    for title, id_sent in zip(facts['title'], facts['sent_id']):\n",
    "        id_title = context['title'].index(title)\n",
    "        if id_sent >= context_length[id_title]:\n",
    "            print(f'id sent not valid in [title={id_title},sent={id_sent}], id={_id}')\n",
    "        else:\n",
    "            facts_mask[id_title][id_sent] = True\n",
    "    \n",
    "    for text, length, mask in zip(context_text, context_length, facts_mask):\n",
    "        data_dict['question'] += [ question ]\n",
    "        data_dict['context'] += [ text ]\n",
    "        data_dict['facts'] += [ mask ]\n",
    "        data_dict['support'] += [any(mask)]\n",
    "        data_dict['answer'] += [row['answer']]\n",
    "        data_dict['level'] += [row['level']]\n",
    "        \n",
    "    idx += 1\n",
    "    \n",
    "pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from data.hotpot_qa.dataset import HotpotNLIDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset hotpot_qa (/Users/dunguyen/Projects/explanation_on_pair_sequences_task/notebooks/../.cache/dataset/hotpot_qa/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5)\n",
      "Formating train: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63312/63312 [00:13<00:00, 4677.13it/s]\n",
      "id sent not valid in [title=7,sent=2], id=5a7b23ca554299042af8f703\n",
      "id sent not valid in [title=6,sent=20], id=5abed6d45542990832d3a0ef\n",
      "id sent not valid in [title=9,sent=3], id=5ab6b2fb5542995eadef0060\n",
      "id sent not valid in [title=9,sent=2], id=5ae0e2df5542990adbacf6b1\n",
      "id sent not valid in [title=1,sent=4], id=5a8d6138554299585d9e37c7\n",
      "id sent not valid in [title=3,sent=52], id=5ab740165542992aa3b8c7fa\n",
      "id sent not valid in [title=8,sent=2], id=5ab2f812554299545a2cfaee\n",
      "id sent not valid in [title=3,sent=52], id=5ae7e8ef5542994a481bbe05\n",
      "id sent not valid in [title=4,sent=4], id=5ab273ee5542997061209606\n",
      "id sent not valid in [title=4,sent=2], id=5a84517355429933447460d5\n",
      "id sent not valid in [title=3,sent=20], id=5a7e5b2455429934daa2fc10\n",
      "id sent not valid in [title=7,sent=40], id=5a846921554299123d8c2243\n",
      "id sent not valid in [title=6,sent=2], id=5add66475542992200553af1\n",
      "id sent not valid in [title=5,sent=2], id=5a847c91554299123d8c2268\n",
      "id sent not valid in [title=5,sent=30], id=5a7b629555429927d897bfa4\n",
      "Formating val: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27135/27135 [00:06<00:00, 4426.46it/s]\n",
      "id sent not valid in [title=7,sent=7], id=5a80577a5542996402f6a4e9\n",
      "id sent not valid in [title=6,sent=8], id=5a8164fb5542995ce29dcbf6\n",
      "id sent not valid in [title=0,sent=2], id=5abe4bb855429965af743eb8\n",
      "id sent not valid in [title=3,sent=11], id=5a90abc355429933b8a2058a\n",
      "id sent not valid in [title=1,sent=2], id=5ab6460b5542995eadeeff96\n",
      "id sent not valid in [title=4,sent=4], id=5a8c7b125542995e66a47614\n",
      "id sent not valid in [title=6,sent=3], id=5a8a317455429930ff3c0cef\n"
     ]
    }
   ],
   "source": [
    "dataset = HotpotNLIDataset(root=path.join(cache_path, 'dataset'), split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['support']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}