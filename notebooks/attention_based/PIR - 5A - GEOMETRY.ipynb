{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> current directory : C:\\Users\\loicf\\Documents\\IRISA\\ExplanationPairSentencesTasks\n",
      ">> cache path : C:\\Users\\loicf\\Documents\\IRISA\\ExplanationPairSentencesTasks\\.cache\n",
      ">> model path : C:\\Users\\loicf\\Documents\\IRISA\\ExplanationPairSentencesTasks\\.cache\\models\n",
      ">> dataset path : C:\\Users\\loicf\\Documents\\IRISA\\ExplanationPairSentencesTasks\\.cache\\dataset\n",
      ">> logs path : C:\\Users\\loicf\\Documents\\IRISA\\ExplanationPairSentencesTasks\\.cache\\logs\n",
      ">> device : cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "cwd = os.getcwd().split(os.path.sep)\n",
    "\n",
    "# point to the git repository\n",
    "while cwd[-1] != \"ExplanationPairSentencesTasks\":\n",
    "    os.chdir(\"..\")\n",
    "    cwd = os.getcwd().split(os.path.sep)\n",
    "print(f\">> current directory : {os.getcwd()}\")\n",
    "\n",
    "# add the root directory\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))\n",
    "\n",
    "# cache and data cache\n",
    "cache_path = path.join(os.getcwd() ,'.cache')\n",
    "dataset_path = path.join(cache_path, 'dataset')\n",
    "log_path = path.join(cache_path, 'logs')\n",
    "model_path = path.join(cache_path, 'models')\n",
    "print(f\">> cache path : {cache_path}\")\n",
    "print(f\">> model path : {model_path}\")\n",
    "print(f\">> dataset path : {dataset_path}\")\n",
    "print(f\">> logs path : {log_path}\")\n",
    "\n",
    "# import the different modules\n",
    "from src.data_module.hatexplain import CLSTokenHateXPlainDM\n",
    "from pur_attention_key_reg import AttitModel\n",
    "from modules import metrics\n",
    "from notebooks.attention_based.utils.ckp_config import *\n",
    "\n",
    "# external librairies\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from modules.metrics.geometry import cosine_sim, effective_rank\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\">> device : {DEVICE}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "%%capture\n",
    "## work on the hatexplain dataset\n",
    "sim_k_dict = {\n",
    "    f\"n_layer={i+1}\" : np.zeros((i+1,)) for i in range(5)\n",
    "}\n",
    "sim_v_dict = {\n",
    "    f\"n_layer={i+1}\" : np.zeros((i+1,)) for i in range(5)\n",
    "}\n",
    "\n",
    "dm_kwargs = dict(cache_path=dataset_path,\n",
    "                 batch_size=32,\n",
    "                 num_workers=0,\n",
    "                 n_data=999\n",
    "                 )\n",
    "\n",
    "dm = CLSTokenHateXPlainDM(**dm_kwargs)\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup(stage=\"test\")\n",
    "test_dataloader = dm.test_dataloader() # load the test dataset\n",
    "\n",
    "model_args = dict(\n",
    "        cache_path=model_path,\n",
    "        mode=\"exp\",\n",
    "        vocab=dm.vocab,\n",
    "        lambda_entropy=0,\n",
    "        lambda_supervise=0,\n",
    "        lambda_lagrange=0,\n",
    "        pretrained_vectors=\"glove.840B.300d\",\n",
    "        num_layers=1,\n",
    "        num_heads=1,\n",
    "        d_embedding=300,\n",
    "        data=\"hatexplain\",\n",
    "        num_class=dm.num_class,\n",
    "        opt=\"adadelta\"\n",
    ")\n",
    "cpt = torch.tensor([0, 0, 0, 0, 0], device=DEVICE)\n",
    "for l in range(5) :\n",
    "\n",
    "    # update the args for the model\n",
    "    model_args[\"num_layers\"] = l+1\n",
    "    ckp = os.path.join(log_path, \"PurAttention\", f\"run=0_hatexplain_l=0{l+1}_h=1_adam\", \"checkpoints\", \"best.ckpt\")\n",
    "    hparams = os.path.join(log_path, \"PurAttention\", f\"run=0_hatexplain_l=0{l+1}_h=1_adam\", \"hparams.yaml\")\n",
    "\n",
    "    # the model\n",
    "    model = AttitModel.load_from_checkpoint(ckp, hparams_file=hparams, **model_args)\n",
    "    model = model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = model.to(DEVICE)\n",
    "        pbar = tqdm(enumerate(test_dataloader), total = int(999/32))\n",
    "        for id_batch, batch in pbar:\n",
    "            pbar.set_description(\"proceed the similarity metric\")\n",
    "            ids = batch[\"token_ids\"].to(DEVICE)\n",
    "            padding_mask = batch[\"padding_mask\"].bool().to(DEVICE)\n",
    "            output = model(ids=ids, mask=padding_mask)\n",
    "            cl = output[\"logits\"].argmax(dim=-1)\n",
    "            cpt[l] += (cl == batch[\"y_true\"].to(DEVICE)).sum().item()\n",
    "            k, v = output[\"key_embeddings\"], output[\"value_embeddings\"]\n",
    "            # ENTROPY : calculation of the entropy on each layer\n",
    "            for i in range(l+1):\n",
    "                # calculus of the metrics\n",
    "                sim_k = cosine_sim(k[i], padding_mask, normalize=\"\")\n",
    "                sim_v = cosine_sim(v[i], padding_mask, normalize=\"\")\n",
    "\n",
    "                sim_k_dict[f\"n_layer={l+1}\"][i] += sim_k.sum().item()\n",
    "                sim_v_dict[f\"n_layer={l+1}\"][i] += sim_v.sum().item()\n",
    "\n",
    "    model = model.cpu()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "for k in sim_k_dict:\n",
    "    sim_k_dict[k] = sim_k_dict[k] / 999\n",
    "    sim_v_dict[k] = sim_v_dict[k] / 999\n",
    ";"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([62.4625, 62.4625, 63.6637, 63.9640, 62.7628])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cpt / 999).cpu() * 100 # the accuracy (in %)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'n_layer=1': array([0.71326747]),\n 'n_layer=2': array([0.69151901, 0.68288404]),\n 'n_layer=3': array([0.69846342, 0.68784525, 0.84177555]),\n 'n_layer=4': array([0.61357826, 0.61971813, 0.74860878, 0.83997919]),\n 'n_layer=5': array([0.62424775, 0.64732869, 0.77722902, 0.91285675, 0.97317868]),\n 'n_layer=6': array([0., 0., 0., 0., 0., 0.])}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_k_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "{'n_layer=1': array([0.99999999]),\n 'n_layer=2': array([1.00000002, 0.99999997]),\n 'n_layer=3': array([0.99999998, 0.99999999, 0.99999999]),\n 'n_layer=4': array([1.        , 0.99999998, 0.99999994, 0.99999998]),\n 'n_layer=5': array([0.99999995, 0.99999999, 0.99999997, 0.99999999, 0.99999997]),\n 'n_layer=6': array([0., 0., 0., 0., 0., 0.])}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_v_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ici une première conclusion est de dire que l'attention ne sert absolument à rien. nous avons des vecteurs de valeurs qui sont tout le temps au même endroit."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}