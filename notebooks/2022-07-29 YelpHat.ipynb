{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42291469-df9d-43bc-8e04-550994f2f37b",
   "metadata": {},
   "source": [
    "# Study on Yelp-Hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7496c4-0dd9-4d9c-9d39-fce827df3bee",
   "metadata": {},
   "source": [
    "Paper: https://davis.wpi.edu/dsrg/PROJECTS/YELPHAT/2020_ACL_Human_vs_Machine-2.pdf\n",
    "\n",
    "Summary:\n",
    "* __Do annotators carefully choose relevant words?__ Yes, as the collecting time and number of chosen words increase accross the sentence length.\n",
    "\n",
    "Sigles:\n",
    "* __HAM__ (Human Attention Map): what annotators denote\n",
    "* __CAM__ (Consensus Attention Map): bitwise __AND__ operation of the HAMs\n",
    "* __SAM__ (Super Attention Map): bitwise __OR__ operation of the HAMs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5a77f3-37ef-489d-bbb7-73be05cfc3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./../src\")\n",
    "\n",
    "cache_path = path.join(os.getcwd(), '..', '.cache')\n",
    "\n",
    "DATASET_NAME='yelp-hat'\n",
    "\n",
    "dataset_path = path.join(cache_path, 'dataset', DATASET_NAME)\n",
    "\n",
    "tmp_path = path.join('.cache', '2022-07-29')\n",
    "os.makedirs(tmp_path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02a562-b02d-4bec-aee5-1951162e0789",
   "metadata": {},
   "source": [
    "Download and extract dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c006c64-6808-42d4-9ae0-394cc633df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import shutil\n",
    "\n",
    "# Download dataset zip\n",
    "URL='https://github.com/cansusen/Human-Attention-for-Text-Classification/archive/205c1552bc7be7ec48623d79d85d4c6fbfe62362.zip'\n",
    "\n",
    "zip_path = download_from_url(URL, root=dataset_path, path=path.join(dataset_path, f'{DATASET_NAME}.zip'))\n",
    "extracted_path = path.join(dataset_path, 'caching')\n",
    "files = extract_archive(from_path=zip_path, to_path=extracted_path)\n",
    "files = [f for f in files if f.endswith('.csv')]\n",
    "\n",
    "for f in files: shutil.copy2(f, extracted_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e15a9b-8e9b-45cf-bde9-81cd12eabafa",
   "metadata": {},
   "source": [
    "A quoi ressemble la dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ef225d-0bbb-4172-8037-55416133d072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input.label</th>\n",
       "      <th>Input.text</th>\n",
       "      <th>Answer.Q1Answer</th>\n",
       "      <th>Answer.html_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Out in Twinsburg for work and wasn't expecting...</td>\n",
       "      <td>yes</td>\n",
       "      <td>&lt;span&gt;Out&lt;/span&gt; &lt;span&gt;in&lt;/span&gt; &lt;span&gt;Twinsbu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Out in Twinsburg for work and wasn't expecting...</td>\n",
       "      <td>yes</td>\n",
       "      <td>&lt;span&gt;Out&lt;/span&gt; &lt;span&gt;in&lt;/span&gt; &lt;span&gt;Twinsbu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Out in Twinsburg for work and wasn't expecting...</td>\n",
       "      <td>yes</td>\n",
       "      <td>&lt;span&gt;Out&lt;/span&gt; &lt;span&gt;in&lt;/span&gt; &lt;span&gt;Twinsbu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Very slow. Never been in the drive at any othe...</td>\n",
       "      <td>no</td>\n",
       "      <td>&lt;span class=\"active\"&gt;Very&lt;/span&gt; &lt;span class=\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Very slow. Never been in the drive at any othe...</td>\n",
       "      <td>no</td>\n",
       "      <td>&lt;span&gt;Very&lt;/span&gt; &lt;span class=\"active\"&gt;slow.&lt;/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0</td>\n",
       "      <td>I went here to get a snack before I went on th...</td>\n",
       "      <td>no</td>\n",
       "      <td>&lt;span&gt;I&lt;/span&gt; &lt;span&gt;went&lt;/span&gt; &lt;span&gt;here&lt;/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0</td>\n",
       "      <td>I went here to get a snack before I went on th...</td>\n",
       "      <td>no</td>\n",
       "      <td>&lt;span&gt;I&lt;/span&gt; &lt;span&gt;went&lt;/span&gt; &lt;span&gt;here&lt;/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0</td>\n",
       "      <td>Always packed for lunch.  Probably because Pit...</td>\n",
       "      <td>no</td>\n",
       "      <td>&lt;span&gt;Always&lt;/span&gt; &lt;span&gt;packed&lt;/span&gt; &lt;span&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0</td>\n",
       "      <td>Always packed for lunch.  Probably because Pit...</td>\n",
       "      <td>no</td>\n",
       "      <td>&lt;span&gt;Always&lt;/span&gt; &lt;span&gt;packed&lt;/span&gt; &lt;span&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0</td>\n",
       "      <td>Always packed for lunch.  Probably because Pit...</td>\n",
       "      <td>no</td>\n",
       "      <td>&lt;span class=\"active\"&gt;Always&lt;/span&gt; &lt;span class...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Input.label                                         Input.text  \\\n",
       "0              1  Out in Twinsburg for work and wasn't expecting...   \n",
       "1              1  Out in Twinsburg for work and wasn't expecting...   \n",
       "2              1  Out in Twinsburg for work and wasn't expecting...   \n",
       "3              0  Very slow. Never been in the drive at any othe...   \n",
       "4              0  Very slow. Never been in the drive at any othe...   \n",
       "..           ...                                                ...   \n",
       "895            0  I went here to get a snack before I went on th...   \n",
       "896            0  I went here to get a snack before I went on th...   \n",
       "897            0  Always packed for lunch.  Probably because Pit...   \n",
       "898            0  Always packed for lunch.  Probably because Pit...   \n",
       "899            0  Always packed for lunch.  Probably because Pit...   \n",
       "\n",
       "    Answer.Q1Answer                                 Answer.html_output  \n",
       "0               yes  <span>Out</span> <span>in</span> <span>Twinsbu...  \n",
       "1               yes  <span>Out</span> <span>in</span> <span>Twinsbu...  \n",
       "2               yes  <span>Out</span> <span>in</span> <span>Twinsbu...  \n",
       "3                no  <span class=\"active\">Very</span> <span class=\"...  \n",
       "4                no  <span>Very</span> <span class=\"active\">slow.</...  \n",
       "..              ...                                                ...  \n",
       "895              no  <span>I</span> <span>went</span> <span>here</s...  \n",
       "896              no  <span>I</span> <span>went</span> <span>here</s...  \n",
       "897              no  <span>Always</span> <span>packed</span> <span>...  \n",
       "898              no  <span>Always</span> <span>packed</span> <span>...  \n",
       "899              no  <span class=\"active\">Always</span> <span class...  \n",
       "\n",
       "[900 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(path.join(extracted_path, 'ham_part1(50words).csv'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14f7a0ed-c9fa-424e-9473-db6c85dce7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words highlighted in this review: 6\n",
      "Original annotation: <span>Out</span> <span>in</span> <span>Twinsburg</span> <span>for</span> <span>work</span> <span>and</span> <span>wasn't</span> <span>expecting</span> <span>to</span> <span>find</span> <span>a</span> <span>well</span> <span>reviewed</span> <span>sushi</span> <span>restaurant</span> <span>but</span> <span class=\"active\">glad</span> <span>I</span> <span>did.</span> <span>It</span> <span>was</span> <span>quite</span> <span>busy</span> <span>for</span> <span>a</span> <span>Monday</span> <span>and</span> <span>the</span> <span>poor</span> <span>waitress</span> <span>was</span> <span>slammed</span> <span>but</span> <span>the</span> <span>sushi</span> <span>chef</span> <span>stepped</span> <span>in</span> <span>to</span> <span>help</span> <span>and</span> <span>was</span> <span class=\"active\">very</span> <span class=\"active\">friendly.</span> <span>The</span> <span class=\"active\">presentation</span> <span>and</span> <span class=\"active\">flavors</span> <span>were</span> <span class=\"active\">great.</span> <span></span>\n",
      "Binarized attention map: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def generate_binary_human_attention_vector(html, num_words_in_review, max_words):\n",
    "    # Function provided by the dataset :\n",
    "    # https://github.com/cansusen/Human-Attention-for-Text-Classification/blob/master/generate_ham/sample_generate.ipynb\n",
    "\n",
    "    p = re.compile('<span(.*?)/span>')\n",
    "    all_span_items = p.findall(html)\n",
    "\n",
    "    if html == '{}':\n",
    "        print('Empty human annotation - This should never print')\n",
    "        return [0] * max_words\n",
    "\n",
    "    if len(all_span_items) == num_words_in_review + 1:\n",
    "        if (all_span_items[num_words_in_review] == '><') or (all_span_items[num_words_in_review] == ' data-vivaldi-spatnav-clickable=\"1\"><'):\n",
    "\n",
    "            binarized_human_attention = [0] * max_words\n",
    "            for i in range(0, len(all_span_items) - 1):\n",
    "                if 'class=\"active\"' in all_span_items[i]:\n",
    "                    binarized_human_attention[i] = 1\n",
    "\n",
    "        else:\n",
    "            print('This should never print.')\n",
    "    else:\n",
    "        print('This should never print.')\n",
    "\n",
    "    return binarized_human_attention\n",
    "\n",
    "MAX_WORDS = 100\n",
    "i = 0\n",
    "html = df['Answer.html_output'][i]\n",
    "num_highlighted = html.count('class=\"active\"')\n",
    "num_words_in_review = len(df['Input.text'][i].split())\n",
    "\n",
    "binarized_human_attention = generate_binary_human_attention_vector(html, num_words_in_review, MAX_WORDS)\n",
    "\n",
    "\n",
    "print(\"Number of words highlighted in this review:\",num_highlighted)\n",
    "print(\"Original annotation:\", html)\n",
    "print(\"Binarized attention map:\",binarized_human_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfc5ee17-0c54-4cdd-a0b1-93fccaffe5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, False, True, False, True]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(html):\n",
    "    p = re.compile(r'<span[^>]*>(.+?)</span>')\n",
    "    return p.findall(html)\n",
    "\n",
    "def human_attention(html):\n",
    "\n",
    "    p = re.compile('<span(.*?)/span>')\n",
    "    all_span_items = p.findall(html)\n",
    "    if all_span_items[-1] == '><': all_span_items = all_span_items[:-1]\n",
    "\n",
    "    return ['class=\"active\"' in span_item for span_item in all_span_items]\n",
    "\n",
    "v_hat = human_attention(html)\n",
    "print(v_hat)\n",
    "\n",
    "# Check if we tokenize html, can our function \"human_attention\" reproduce the exact same length\n",
    "for fpath in os.listdir(dataset_path):\n",
    "    \n",
    "    if fpath.endswith('.csv'):\n",
    "        print('Check',fpath)\n",
    "        df = pd.read_csv(path.join(dataset_path, fpath))\n",
    "        print(len(df['Answer.html_output']))\n",
    "        for html in df['Answer.html_output']:\n",
    "            tokens = tokenize(html)\n",
    "            v_hat = human_attention(html)\n",
    "            if len(tokens) != len(v_hat):\n",
    "                print(len(tokens), len(v_hat))\n",
    "                print(' '.tokens)\n",
    "                display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a294fbb9-118e-4076-a792-9bfd59f1415c",
   "metadata": {},
   "source": [
    "Proceed dataset step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7502c20-2195-4bf3-b8a6-2997d002e9c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Raw dataset</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input.label</th>\n",
       "      <th>Input.text</th>\n",
       "      <th>Answer.Q1Answer</th>\n",
       "      <th>Answer.html_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't think so. \\n\\nThis \"buffet\" is probabl...</td>\n",
       "      <td>no</td>\n",
       "      <td>&lt;span&gt;I&lt;/span&gt; &lt;span&gt;don't&lt;/span&gt; &lt;span&gt;think&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't think so. \\n\\nThis \"buffet\" is probabl...</td>\n",
       "      <td>no</td>\n",
       "      <td>&lt;span&gt;I&lt;/span&gt; &lt;span class=\"active\"&gt;don't&lt;/spa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't think so. \\n\\nThis \"buffet\" is probabl...</td>\n",
       "      <td>no</td>\n",
       "      <td>&lt;span&gt;I&lt;/span&gt; &lt;span&gt;don't&lt;/span&gt; &lt;span&gt;think&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Ate here two times now and food has been excel...</td>\n",
       "      <td>yes</td>\n",
       "      <td>&lt;span&gt;Ate&lt;/span&gt; &lt;span&gt;here&lt;/span&gt; &lt;span&gt;two&lt;/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Ate here two times now and food has been excel...</td>\n",
       "      <td>yes</td>\n",
       "      <td>&lt;span&gt;Ate&lt;/span&gt; &lt;span&gt;here&lt;/span&gt; &lt;span&gt;two&lt;/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Input.label                                         Input.text  \\\n",
       "0            0  I don't think so. \\n\\nThis \"buffet\" is probabl...   \n",
       "1            0  I don't think so. \\n\\nThis \"buffet\" is probabl...   \n",
       "2            0  I don't think so. \\n\\nThis \"buffet\" is probabl...   \n",
       "3            1  Ate here two times now and food has been excel...   \n",
       "4            1  Ate here two times now and food has been excel...   \n",
       "\n",
       "  Answer.Q1Answer                                 Answer.html_output  \n",
       "0              no  <span>I</span> <span>don't</span> <span>think<...  \n",
       "1              no  <span>I</span> <span class=\"active\">don't</spa...  \n",
       "2              no  <span>I</span> <span>don't</span> <span>think<...  \n",
       "3             yes  <span>Ate</span> <span>here</span> <span>two</...  \n",
       "4             yes  <span>Ate</span> <span>here</span> <span>two</...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'duplicates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m display(HTML(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<h3>Raw dataset</h3>\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      9\u001b[0m display(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m---> 11\u001b[0m dupli_2 \u001b[38;5;241m=\u001b[39m \u001b[43mduplicates\u001b[49m[duplicates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# finds dupli_2\u001b[39;00m\n\u001b[1;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;241m~\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput.text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(dupli_2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m     14\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mham_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer.html_output\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: yelp_hat_ham(x, nlp))\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'duplicates' is not defined"
     ]
    }
   ],
   "source": [
    "from data.yelp_hat.utils import yelp_hat_ham, yelp_hat_token\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "df = pd.read_csv(path.join(extracted_path, 'ham_part5.csv'))\n",
    "display(HTML('<h3>Raw dataset</h3>'))\n",
    "display(df.head())\n",
    "\n",
    "dupli_2 = duplicates[duplicates['size'] == 2]  # finds dupli_2\n",
    "df = df[~df['Input.text'].isin(dupli_2['index'])]\n",
    "\n",
    "df['ham_'] = df[f'Answer.html_output'].apply(lambda x: yelp_hat_ham(x, nlp)).apply(lambda x: np.array(x))\n",
    "df['text_tokens'] = df['Answer.html_output'].apply(lambda x: yelp_hat_token(x, nlp))\n",
    "df = df.rename(columns={'Answer.Q1Answer': 'human_label_', 'Input.text': 'text', 'Input.label': 'label'})\n",
    "\n",
    "display(HTML('<h3>Preprocess text and human attention</h3>'))\n",
    "display(df.head())\n",
    "\n",
    "dfs = [df.loc[0::3, ['text', 'label', 'text_tokens']].reset_index(drop=True)]\n",
    "for idx in range(3):\n",
    "    _data = df.loc[idx::3, ['ham_', 'human_label_']]\n",
    "    _data = _data.reset_index(drop=True).add_suffix(str(idx))\n",
    "    dfs += [_data]\n",
    "    \n",
    "df = pd.concat(dfs, axis=1)\n",
    "\n",
    "def cam(row):\n",
    "    return np.logical_and(row['ham_0'],row['ham_1'],row['ham_2'])\n",
    "\n",
    "def sam(row):\n",
    "    return np.logical_or(row['ham_0'],row['ham_1'],row['ham_2'])\n",
    "\n",
    "def ham(row):\n",
    "    return ((row['ham_0'] + row['ham_1'] + row['ham_2'])/3 >= 0.5).astype(int)\n",
    "\n",
    "df['cam'] = df.apply(cam,axis=1)\n",
    "df['sam'] = df.apply(sam,axis=1)\n",
    "df['ham'] = df.apply(ham,axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60eca72-76f0-44a1-b9ba-2ae981593576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path.join(extracted_path, 'ham_part5.csv'))\n",
    "duplicates = df.groupby(df['Input.text'].tolist(),as_index=False).size()\n",
    "display(duplicates['size'].unique())\n",
    "for dup_rows in [2, 3]:\n",
    "    print('Duplicate of', dup_rows, ':', sum(duplicate['size'] == dup_rows))\n",
    "    \n",
    "dupli_2 = duplicates[duplicates['size'] == 2]  # finds dupli_2\n",
    "df = df[~df['Input.text'].isin(dupli_2['index'])]\n",
    "duplicate = df.groupby(df['Input.text'].tolist(),as_index=False).size()\n",
    "display(duplicate['size'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52565e89-3f93-4e02-a032-5a62f40875a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['coherent_ham_01'] = df['ham_0'].str.len() == df['ham_1'].str.len()\n",
    "df['coherent_ham_12'] = df['ham_1'].str.len() == df['ham_2'].str.len()\n",
    "print(df['coherent_ham_01'].all())\n",
    "print(df['coherent_ham_12'].all())\n",
    "print('#Incoherent length:', df['coherent_ham_12'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3afc25-fec6-4967-a7cb-2140c2846b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam(row):\n",
    "    return np.logical_and(row['ham_0'],row['ham_1'],row['ham_2'])\n",
    "\n",
    "def sam(row):\n",
    "    return np.logical_or(row['ham_0'],row['ham_1'],row['ham_2'])\n",
    "\n",
    "def ham(row):\n",
    "    return ((row['ham_0'] + row['ham_1'] + row['ham_2'])/3 >= 0.5).astype(int)\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    df = pd.read_csv(f)\n",
    "    df['ham_'] = df[f'Answer.html_output'].apply(yelp_hat_ham).apply(lambda x: np.array(x))\n",
    "    df['text_tokens'] = df['Answer.html_output'].apply(yelp_hat_token)\n",
    "    df = df.rename(columns={'Answer.Q1Answer': 'human_label_', 'Input.text': 'text', 'Input.label': 'label'})\n",
    "    dfs = [df.loc[0::3, ['text', 'label', 'text_tokens']].reset_index(drop=True)]\n",
    "    for idx in range(3):\n",
    "        _data = df.loc[idx::3, ['ham_', 'human_label_']]\n",
    "        _data = _data.reset_index(drop=True).add_suffix(str(idx))\n",
    "        dfs += [_data]\n",
    "\n",
    "    df = pd.concat(dfs, axis=1)\n",
    "\n",
    "    try:\n",
    "        df['cam'] = df.apply(cam,axis=1)\n",
    "        df['sam'] = df.apply(sam,axis=1)\n",
    "        df['ham'] = df.apply(ham,axis=1)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29102c3-1beb-49d3-b44a-7b67a5ab59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cam(row):\n",
    "    return np.logical_and(row['ham_0'],row['ham_1'],row['ham_2'])\n",
    "\n",
    "def sam(row):\n",
    "    return np.logical_or(row['ham_0'],row['ham_1'],row['ham_2'])\n",
    "\n",
    "def ham(row):\n",
    "    return ((row['ham_0'] + row['ham_1'] + row['ham_2'])/3 >= 0.5).astype(int)\n",
    "\n",
    "df['cam'] = df.apply(cam,axis=1)\n",
    "df['sam'] = df.apply(sam,axis=1)\n",
    "df['ham'] = df.apply(ham,axis=1)\n",
    "df.head()\n",
    "# sam = df.apply(lambda x: np.logical_and(np.array(x['ham_0']), np.array(x['ham_1']), np.array(x['ham_2'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68cd16-6e28-45fd-b86a-6cba7f3819f1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> By reconcatenating for spacy and re tokenize, do we obtain the coherent length?\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8fca69a6-c3f6-4d57-afa9-07eef4f35481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463d6d78fa344beebf821c878c32b9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham_part5.csv : Same tokens between text and ham > True\n",
      "ham_part8(200words).csv : Same tokens between text and ham > True\n",
      "ham_part6(100words).csv : Same tokens between text and ham > True\n",
      "ham_part4.csv : Same tokens between text and ham > True\n",
      "ham_part3.csv : Same tokens between text and ham > True\n",
      "ham_part1(50words).csv : Same tokens between text and ham > True\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "def tokenize_ham(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = [str(tag.string) for tag in soup.find_all('span') if tag.string is not None]\n",
    "    tokens = [str(tk.text) for doc in nlp.pipe(tags) for tk in doc]\n",
    "    return tokens\n",
    "\n",
    "files = [fpath for fpath in os.listdir(extracted_path) if fpath.endswith('.csv') and 'part7' not in fpath]\n",
    "\n",
    "for fpath in tqdm(files, total=len(files)):\n",
    "        \n",
    "    df = pd.read_csv(path.join(extracted_path, fpath))\n",
    "\n",
    "    dfs = [df.loc[0::3, df.columns != 'Answer.html_output'].reset_index(drop=True)]\n",
    "    dfs += [df.loc[idx::3, ['Answer.html_output']].reset_index(drop=True).rename(columns={'Answer.html_output':'ham_html_'}).add_suffix(str(idx)) for idx in range(3) ]\n",
    "    clean_df = pd.concat(dfs,axis=1).rename(columns={'Input.label': 'label', 'Input.text': 'text', 'Answer.Q1Answer':'human_label'})\n",
    "    clean_df = clean_df[['text', 'ham_html_0', 'ham_html_1', 'ham_html_2', 'label', 'human_label']]\n",
    "\n",
    "    clean_df['ham_tokens'] = clean_df['ham_html_0'].apply(tokenize_ham)\n",
    "    clean_df['count_ham_tokens'] = clean_df['ham_tokens'].apply(lambda row: len(row))\n",
    "\n",
    "    clean_df['text_tokens'] = [[tk.text for tk in doc if not tk.is_space] for doc in nlp.pipe(clean_df['text'].tolist())]\n",
    "    clean_df['count_text_tokens'] = clean_df['text_tokens'].apply(lambda row: len(row))\n",
    "\n",
    "    is_every_row_ok = (clean_df['count_text_tokens'] == clean_df['count_ham_tokens']).all()\n",
    "    print(fpath, ': Same tokens between text and ham >', is_every_row_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d68d5-ab3f-40fd-9260-02610908f91c",
   "metadata": {},
   "source": [
    "###### Split tokens in human attention maps by spacy, verify if `len(token) == len(annotation)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc1689-a576-427d-a481-e67541fb0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_ham(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = [tag for tag in soup.find_all('span') if tag.string is not None]\n",
    "    \n",
    "    tag_annot = [int('active' in t.get('class', [])) for t in tags]\n",
    "    tag_str = [str(t.string) for t in tags]\n",
    "    \n",
    "    ham = []\n",
    "    \n",
    "    for annot, splitted_tokens in zip(tag_annot, nlp.pipe(tag_str)):\n",
    "        annotation = [annot * int(not tk.is_punct) for tk in splitted_tokens]\n",
    "        ham += annotation\n",
    "    \n",
    "    return ham\n",
    "\n",
    "html = clean_df['ham_html_0']\n",
    "print(html[0])\n",
    "print(tokenize_ham(html[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a068205-9890-40c3-892e-16fd77d2c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [fpath for fpath in os.listdir(dataset_path) if fpath.endswith('.csv') and 'part7' not in fpath]\n",
    "\n",
    "for fpath in tqdm(files, total=len(files)):\n",
    "        \n",
    "    df = pd.read_csv(path.join(dataset_path, fpath))\n",
    "\n",
    "    dfs = [df.loc[0::3, df.columns != 'Answer.html_output'].reset_index(drop=True)]\n",
    "    dfs += [df.loc[idx::3, ['Answer.html_output']].reset_index(drop=True).rename(columns={'Answer.html_output':'annotation_html_'}).add_suffix(str(idx)) for idx in range(3) ]\n",
    "    clean_df = pd.concat(dfs,axis=1).rename(columns={'Input.label': 'label', 'Input.text': 'text', 'Answer.Q1Answer':'human_label'})\n",
    "    clean_df = clean_df[['text', 'ham_html_0', 'ham_html_1', 'ham_html_2', 'label', 'human_label']]\n",
    "\n",
    "    clean_df['ham_tokens'] = clean_df['ham_html_0'].apply(binarize_ham)\n",
    "    clean_df['count_ham_tokens'] = clean_df['ham_tokens'].apply(lambda row: len(row))\n",
    "\n",
    "    clean_df['text_tokens'] = [[tk.text for tk in doc if not tk.is_space] for doc in nlp.pipe(clean_df['text'].tolist())]\n",
    "    clean_df['count_text_tokens'] = clean_df['text_tokens'].apply(lambda row: len(row))\n",
    "\n",
    "    is_every_row_ok = (clean_df['count_text_tokens'] == clean_df['count_ham_tokens']).all()\n",
    "    print(fpath, ': Same tokens between text and ham >', is_every_row_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37284a6-44d4-4c46-a765-2ee421f48cfd",
   "metadata": {},
   "source": [
    "## Check sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76b9d553-01e7-49a1-8b7f-1ed61696d345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a18de6908874463b30b42b5825df838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>ham_part5.csv</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.499994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_length</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>71.694</td>\n",
       "      <td>7.983865</td>\n",
       "      <td>55.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count    mean       std   min   25%   50%   75%   max\n",
       "label        1000.0   0.516  0.499994   0.0   0.0   1.0   1.0   1.0\n",
       "text_length  1000.0  71.694  7.983865  55.0  66.0  71.0  78.0  97.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>ham_part8(200words).csv</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>181.0</td>\n",
       "      <td>0.508287</td>\n",
       "      <td>0.501318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_length</th>\n",
       "      <td>181.0</td>\n",
       "      <td>230.071823</td>\n",
       "      <td>8.317340</td>\n",
       "      <td>210.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count        mean       std    min    25%    50%    75%    max\n",
       "label        181.0    0.508287  0.501318    0.0    0.0    1.0    1.0    1.0\n",
       "text_length  181.0  230.071823  8.317340  210.0  224.0  230.0  235.0  252.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>ham_part6(100words).csv</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>438.0</td>\n",
       "      <td>0.513699</td>\n",
       "      <td>0.500384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_length</th>\n",
       "      <td>438.0</td>\n",
       "      <td>115.276256</td>\n",
       "      <td>5.565827</td>\n",
       "      <td>101.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count        mean       std    min    25%    50%    75%    max\n",
       "label        438.0    0.513699  0.500384    0.0    0.0    1.0    1.0    1.0\n",
       "text_length  438.0  115.276256  5.565827  101.0  112.0  114.0  118.0  144.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>ham_part4.csv</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.500201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_length</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>71.604</td>\n",
       "      <td>7.837641</td>\n",
       "      <td>55.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count    mean       std   min   25%   50%   75%    max\n",
       "label        1000.0   0.507  0.500201   0.0   0.0   1.0   1.0    1.0\n",
       "text_length  1000.0  71.604  7.837641  55.0  65.0  71.0  78.0  101.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>ham_part3.csv</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.500241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_length</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>71.958</td>\n",
       "      <td>8.178944</td>\n",
       "      <td>53.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count    mean       std   min   25%   50%   75%    max\n",
       "label        1000.0   0.497  0.500241   0.0   0.0   0.0   1.0    1.0\n",
       "text_length  1000.0  71.958  8.178944  53.0  65.0  72.0  78.0  107.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>ham_part1(50words).csv</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>300.0</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.500557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_length</th>\n",
       "      <td>300.0</td>\n",
       "      <td>58.006667</td>\n",
       "      <td>3.009454</td>\n",
       "      <td>50.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count       mean       std   min   25%   50%   75%   max\n",
       "label        300.0   0.483333  0.500557   0.0   0.0   0.0   1.0   1.0\n",
       "text_length  300.0  58.006667  3.009454  50.0  56.0  57.0  60.0  72.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for fpath in tqdm(files, total=len(files)):\n",
    "        \n",
    "    df = pd.read_csv(path.join(dataset_path, fpath))\n",
    "    display(HTML(f'<h3>{fpath}</h3>'))\n",
    "    dfs = [df.loc[0::3, df.columns != 'Answer.html_output'].reset_index(drop=True)]\n",
    "    dfs += [df.loc[idx::3, ['Answer.html_output']].reset_index(drop=True).rename(columns={'Answer.html_output':'ham_html_'}).add_suffix(str(idx)) for idx in range(3) ]\n",
    "    clean_df = pd.concat(dfs,axis=1).rename(columns={'Input.label': 'label', 'Input.text': 'text', 'Answer.Q1Answer':'human_label'})\n",
    "    clean_df = clean_df[['text', 'ham_html_0', 'ham_html_1', 'ham_html_2', 'label', 'human_label']]\n",
    "    clean_df['text_tokens'] = [[tk.text for tk in doc if not tk.is_space] for doc in nlp.pipe(clean_df['text'].tolist())]\n",
    "    \n",
    "    clean_df['text_length'] = clean_df.text_tokens.str.len()\n",
    "    display(clean_df.describe().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8937c06-84db-443f-9e13-a37bcf249a12",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd404f7-0f17-4dc2-a082-31be562f11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_module.yelp_hat import YelpHatDM\n",
    "dm = YelpHatDM(cache_path=path.join(cache_path, 'dataset'), batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7775f39-d526-4dd0-9865-8ab4eb499d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "dataloader = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3595b84c-649c-4cdb-baaa-6bf9aace0acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i be there for lunch last wednedsay base on all the review . i be craving pasta , and that be the only thing i order . their home make spicy pasta sauce be ok . probably because of lunch time , there be no one except for we ( 12:00 till 1:00 ) for the price of the pasta , the quality of the pasta be just a little well than il fornello but for a little bit more .',\n",
       " \"brother 's have always be an excellent place for a donair . it be worth my drive from the ne . the sweet sauce be about as close as i can get to the halifax donair . however , it be close now due to the southwest ctrain route construction . there be a sign on the door say it be close until the owner can find a new location . i sure do hope they can relocate . <pad> <pad>\",\n",
       " 'the all you can eat option be quite good . however , the woman in charge ( manager perhaps ) come and tell we last round for order just before an hour and twenty minute into the meal . also , she keep glare at we the entire time , as though we be eat too much ( 6 guy ) ; rather disrespectful . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>',\n",
       " 'attend the pre grand open saturday night here and everything be pretty decent . from the food , the service , to the decor for the place , everything be handle very well . as this be the pre opening , i will update this review at a later time to give a more accurate review on food and service . cheer ! <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(dataloader))\n",
    "tokens = [dm.vocab.lookup_tokens(token_ids.tolist()) for token_ids in b['token_ids']]\n",
    "[' '.join(tk) for tk in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2686959-d441-49af-a831-5cea5cc67638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The all you can eat option is quite good. However, the woman in charge (manager perhaps) came and told us last rounds for orders just before an hour and twenty minutes into the meal. Also, she kept glaring at us the entire time, as though we were eating too much (6 guys); rather disrespectful.',\n",
       " \"Can't give this place any stars. The people that gave this place any stars have probably never been to a nice stip club. This place was dirty! Downstairs felt like a third world country brothel and upstairs was just way too much going on.  Tons of strippers on the floor which wouldn't be bad if this joint strived for quality and not quantity.\",\n",
       " \"This place had been around for a really long time, I was kind of disappointed when they brought the food out to the table, it didn't look that good and the taste just wasn't there, all in all I wasn't that happy with everything (service, food, price). With so many places in newmarket to choose from, I won't be going back there again\",\n",
       " 'Staff is totally incompetent. Claimed they tried to call to confirm the order before making it yet they send you a confirmation of your order. Ordered online and they claim this was the policy which they made up to cover their ass for their incompetence. \\n\\nThe lady was extremely rude to us on the phone and offered no apology. Would avoid this place like the plague.',\n",
       " \"Devin, Owner of Calistro California Bistro, \\n\\nIf you're still cooking away, why don't you take note of the troubling feedback from the other disappointed patrons?  I love how you only respond to one reviewer and lucky for me, I just so happen to get the pleasure of hearing your disrespectful and abusive attitude. Wow, some people and places never change...\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_set.data.loc[:4, 'text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba972ac-979d-4597-ae7b-5d700354b209",
   "metadata": {},
   "source": [
    "## Choice for vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c5a7399-debf-4204-86b6-ca396a518092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length vocab in original form: 1018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd542e934cba41e6a34b7879ce4d7841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building vocabulary:   0%|          | 0/8 [00:00<?, ?sents/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length for lower vocab: 1018\n",
      "Length for lower lemma vocab: 54\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Original\n",
    "print('Length vocab in original form:',len(dm.vocab))\n",
    "\n",
    "# Lower\n",
    "PAD_TOK = '<pad>'\n",
    "UNK_TOK = '<unk>'\n",
    "\n",
    "dp = dm.train_set.batch(8).map(lambda batch: {k: [row[k] for row in batch] for k in batch[0]}).map(lambda batch: [token.lower() for sentence in batch['text_tokens'] for token in sentence])\n",
    "iter_tokens = tqdm(iter(dp), desc='Building vocabulary', total=len(dp), unit='sents', file=sys.stdout)\n",
    "lower_vocab = build_vocab_from_iterator(iterator=iter_tokens, specials=[PAD_TOK, UNK_TOK])\n",
    "\n",
    "print('Length for lower vocab:',len(lower_vocab))\n",
    "\n",
    "# Lower lemma\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokens = [tk for doc in dm.train_set.data['text_tokens'] for tk in doc]\n",
    "doc = spacy.tokens.Doc(nlp.vocab, words=tokens)\n",
    "lemmatized = [tk.lemma_ for tk in nlp(doc)]\n",
    "lemma_vocab = build_vocab_from_iterator(iterator=lemmatized, specials=[PAD_TOK, UNK_TOK])\n",
    "print('Length for lower lemma vocab:',len(lemma_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e69f2a-4e98-4b11-a1b9-85fafc977fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
